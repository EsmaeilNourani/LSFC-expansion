{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmarking Dictionary-based NER (Tagger) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* First we need to run tagger on the annoated benchmark abstracts and get the list of tagged LSFs to compare them with manually tagged LSFs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tagger run\n",
    "1. prepare abstracts in a fomrat that tagger request\n",
    "    * 6 column tabseparated format, first column(PMID) and the last column which is the text, others can be left as empty since ther are relevant or required here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output file created: ../../data/NER-Benchmarking/annotations/abstracts.tsv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Input directory containing text files\n",
    "input_directory = '../../data/NER-Benchmarking/annotations/raw/'\n",
    "\n",
    "# Output file name\n",
    "output_file = '../../data/NER-Benchmarking/annotations/abstracts.tsv'\n",
    "\n",
    "# List all .txt files in the input directory\n",
    "input_files = [file for file in os.listdir(input_directory) if file.endswith('.txt')]\n",
    "\n",
    "# Open the output file for writing\n",
    "with open(output_file, 'w') as out_file:\n",
    "    for input_file in input_files:\n",
    "        with open(os.path.join(input_directory, input_file), 'r') as in_file:\n",
    "            content = in_file.read().replace('\\n', ' ').strip()  # Read, replace newlines with spaces, and strip any leading/trailing whitespace\n",
    "\n",
    "        # Extract the base name of the input file without the .txt extension\n",
    "        input_filename = os.path.splitext(os.path.basename(input_file))[0]\n",
    "\n",
    "        # Write the formatted line to the output file\n",
    "        line = f\"PMID:{input_filename}\\t \\t \\t \\t \\t{content}\\n\"  # Four empty columns\n",
    "        out_file.write(line)\n",
    "\n",
    "print(\"Output file created:\", output_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Prepare required input files for tagger\n",
    "    * Note: You can use the saved results, or reproduce using the following steps\n",
    "    \n",
    "    * Steps \n",
    "        * . Using required scripts for Tagger prepare required input files for tagger\n",
    "            * tagger scripts ('../NER-Benchmarking/scripts/tagger_scripts/')\n",
    "            * Clone LSFC (or use the offline version in '../../LSFC/LSFC.obo')\n",
    "            * Generate entities,groups,names using LSFC\n",
    "                * ../NER-Benchmarking/scripts/tagger_scripts/obo2reflect.pl  ../../LSFC/   ../../data/NER-Benchmarking/tagger/tagger_input/\n",
    "            *  Add adjective and plural endings (orthoexpand)\n",
    "                * ../NER-Benchmarking/scripts/tagger_scripts/orthoexpand.pl  ../../data/NER-Benchmarking/tagger/tagger_input/LSFC_entities.tsv  ../../data/NER-Benchmarking/tagger/tagger_input/LSFC_names.tsv >  ../../data/NER-Benchmarking/tagger/tagger_input/LSFC_entities_names_expanded_mid.tsv\n",
    "\n",
    "            *  Change nonASCII into ASCII\n",
    "                * cat ../../data/NER-Benchmarking/tagger/tagger_input/LSFC_entities_names_expanded_mid.tsv | python2  ../NER-Benchmarking/scripts/tagger_scripts/utf8expand.py > ../../data/NER-Benchmarking/tagger/tagger_input/LSFC_names_expanded.tsv\n",
    "\n",
    "\n",
    "            * orthounexpand\n",
    "                *  ../NER-Benchmarking/scripts/tagger_scripts/orthounexpand.pl ../../data/NER-Benchmarking/tagger/tagger_input/LSFC_entities.tsv  ../../data/NER-Benchmarking/tagger/tagger_input/LSFC_names.tsv  ../../data/NER-Benchmarking/tagger/tagger_input/LSFC_names_expanded.tsv >  ../../data/NER-Benchmarking/tagger/tagger_input/LSFC_names_unexpanded.tsv\n",
    "\n",
    "\n",
    "            * disambiguate\n",
    "                *  ../NER-Benchmarking/scripts/tagger_scripts/disambiguate.pl ../../data/NER-Benchmarking/tagger/tagger_input/LSFC_names_unexpanded.tsv >  ../../data/NER-Benchmarking/tagger/tagger_input/LSFC_names_disambiguated.tsv\n",
    "\n",
    "            * Create manually (or copy) LSFC_types.tsv ('../../data/NER-Benchmarking/tagger/tagger_input/LSFC_types.tsv')\n",
    "                * which contains types ids: in this case only -20 for life style \n",
    "            \n",
    "            * createing block list file:\n",
    "                * craete shortlist for manual checking\n",
    "                    * block_list_candidates=input_data[input_data.match_count >2000]\n",
    "                * Evalualte candidates and create final block list by concatinating them with general list of blocked words\n",
    "                    * The format of the blocklist file is two tab separated columns:\n",
    "                    * the word, which may be a string containing spaces either the string \"t\" or the string \"f\", according to whether it is a stopword (\"t\") or is whitelisted (\"f\")\n",
    "                    * (../../data/NER-Benchmarking/tagger/tagger_input/all_global.tsv)\n",
    "\n",
    "3. Run Tagger\n",
    "    * Output file : '../../data/NER-Benchmarking/tagger/tagger_output/all_matches_benchmark.tsv'\n",
    "    * We want to run Tagger on small set of annotated documents (200 abstracts) unlike normal situations which is for all pubmed/PMC articles and is a large scale run and takes time \n",
    "        /tagger/tagcorpus \n",
    "        --documents=../../data/NER-Benchmarking/annotations/abstracts.tsv\n",
    "        --threads=1 \\\n",
    "        --autodetect \\\n",
    "        --types=tagger_input/LSFC_types.tsv \\\n",
    "        --entities=tagger_input/LSFC_entities.tsv \\\n",
    "        --names=tagger_input/LSFC_names_disambiguated.tsv \\\n",
    "        --stopwords=tagger_input/all_global.tsv \\\n",
    "        --local-stopwords=tagger_input/all_local.tsv \\\n",
    "        --groups=tagger_input/LSFC_groups.tsv \\\n",
    "        --out-matches=tagger_output/all_matches_benchmark.tsv \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Post processing tagger output\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Tagger does not aasign category to the tagged entities and here we replace serial number with the LSF category name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load LSFC \n",
    "\n",
    "import warnings,sys\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "module_path = os.path.abspath(os.path.join('../../'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path+\"//utils\")\n",
    "\n",
    "import retrieve_LSFC\n",
    "\n",
    "import importlib\n",
    "# reload if library gets updated\n",
    "importlib.reload(retrieve_LSFC)\n",
    "\n",
    "### Load LSFC \n",
    "\n",
    "LSFC_file='../../LSFC/LSFC.obo'\n",
    "id_to_name,name_to_id,id_to_synonyms,id_2_childs,id_2_parents=retrieve_LSFC.read_LSFC(LSFC_file)\n",
    "# Get names of 9 main LSF catgeories\n",
    "LSF_exisiting_names, LSF_Labels,LFIDs,categories=retrieve_LSFC.generate_lfid_categories_labels(LSFC_file)\n",
    "\n",
    "# to make it consistent with annotation attributes\n",
    "categories=['Beauty_and_Cleaning','Nutrition','Drugs','Environmental_exposures','Non_physical_leisure_time_activities','Physical_activity','Sleep','Socioeconomic_factors','Mental_health_practices']\n",
    "# How to extract related ontology classes using Bioportal\n",
    "lfid_to_category={}\n",
    "for i,lfid in enumerate(LFIDs):\n",
    "    lfid_to_category[lfid]=categories[LSF_Labels[i]]\n",
    "\n",
    "\n",
    "#a dictionary to store the serial number of the detected entities and the corresponding lsf category\n",
    "serial_to_cargeory={}\n",
    "tagger_entities=pd.read_csv('../../data/NER-Benchmarking/tagger/tagger_input/LSFC_entities.tsv',sep='\\t')\n",
    "tagger_entities.columns=['serial','entity_type','lfid']\n",
    "for i,row in tagger_entities.iterrows():\n",
    "    if row['lfid']=='LFID:0000000':\n",
    "        continue\n",
    "    serial_to_cargeory[row['serial']]=lfid_to_category[row['lfid']]\n",
    "\n",
    "\n",
    "# load tagger matches  \n",
    "tagger_matches_benchmark=pd.read_csv('../../data/NER-Benchmarking/tagger/tagger_output/all_matches_benchmark.tsv',sep='\\t',header=None)\n",
    "\n",
    "for index, row in tagger_matches_benchmark.iterrows():\n",
    "    \n",
    "    serial = row[7]\n",
    "    if serial==1000000001:\n",
    "        continue\n",
    "    #replace serial number with the corresponding lsf category\n",
    "    tagger_matches_benchmark.at[index,7] = serial_to_cargeory[serial]\n",
    "\n",
    "tagger_matches_benchmark.to_csv('../../data/NER-Benchmarking/tagger/tagger_output/all_matches_benchmark_with_branches.tsv',sep='\\t',header=None,index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Remove possible duplicate lines(some lines were redundant) the following file is saved after removing duplications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "raw_input_file_path = '../../data/NER-Benchmarking/tagger/tagger_output/all_matches_benchmark_with_branches.tsv'\n",
    "\n",
    "deduplicated_input_file_path = '../../data/NER-Benchmarking/tagger/tagger_output/all_matches_benchmark_with_branches_deduplicated.tsv'\n",
    "\n",
    "# Read the input file, remove duplicates, and write to the output file\n",
    "\n",
    "df=pd.read_csv(raw_input_file_path,sep='\\t',header=None)\n",
    "df.drop_duplicates(inplace=True)\n",
    "df.to_csv(deduplicated_input_file_path,index=False,header=None,sep='\\t')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Generate BRAT files using tagger results \n",
    "    * this makes it possible to compare the performance of the *.ann files produced by tagger and annotated by annotator and generate the perfomance result  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy raw *.txt files of the abstracts\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "def copy_txt_files(source_dir, target_dir):\n",
    "    # Create the target directory if it doesn't exist\n",
    "    if not os.path.exists(target_dir):\n",
    "        os.makedirs(target_dir)\n",
    "\n",
    "    for filename in os.listdir(source_dir):\n",
    "        source_path = os.path.join(source_dir, filename)\n",
    "        if os.path.isfile(source_path) and filename.endswith('.txt'):\n",
    "            target_path = os.path.join(target_dir, filename)\n",
    "            shutil.copy2(source_path, target_path)  # Using copy2 to preserve metadata\n",
    "\n",
    "source_directory = \"../../data/NER-Benchmarking/annotations/raw/\"\n",
    "target_directory = \"../../data/NER-Benchmarking/tagger/tagger_output/brat_files/\"\n",
    "\n",
    "copy_txt_files(source_directory, target_directory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "#Produces *.ann files for the abstracts based on the tagger matches \n",
    "awk -F'\\t' '{printf (\"%s\\t%s\\t%s\\t%s\\n\", $4, $5, $6, $7) >> \"../../data/NER-Benchmarking/tagger/tagger_output/brat_files/\"$1\".tsv\"; close(\"../../data/NER-Benchmarking/tagger/tagger_output/brat_files/\"$1\".tsv\")}' ../../data/NER-Benchmarking/tagger/tagger_output/all_matches_benchmark_with_branches_deduplicated.tsv\n",
    "\n",
    "cd ../../data/NER-Benchmarking/tagger/tagger_output/brat_files/\n",
    "\n",
    "# Add incremental indexing to files produced\n",
    "for f in *.tsv\n",
    "do\n",
    "    awk -F \"\\t\" '{$1=++i FS $1; printf(\"T%s\\n\",$0)}' OFS=\"\\t\" $f > \"$(basename \"$f\" .tsv).tsv2\";\n",
    "    rm ${f}\n",
    "done\n",
    "\n",
    "# Make file in ann format\n",
    "for f in *.tsv2\n",
    "do\n",
    "     awk -F \"\\t\" '{if($5~/-20/){printf(\"%s\\tLifestyle_factor %s %s\\t%s\\n\", $1, $2, ++$3, $4)}else{printf(\"%s\\tDisease %s %s\\t%s\\n\", $1, $2, ++$3, $4)}}' OFS=\"\\t\" $f > \"$(basename \"$f\" .tsv2).ann\";\n",
    "     rm ${f}\n",
    "done\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Ceate empty *.ann files for those abstracts that tagger has not found any matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def create_missing_ann_files(source_dir):\n",
    "    for filename in os.listdir(source_dir):\n",
    "        if filename.endswith('.txt'):\n",
    "            txt_file_path = os.path.join(source_dir, filename)\n",
    "            ann_file_path = os.path.splitext(txt_file_path)[0] + '.ann'\n",
    "            \n",
    "            if not os.path.exists(ann_file_path):\n",
    "                with open(ann_file_path, 'w') as ann_file:\n",
    "                    pass  # Creates an empty .ann file\n",
    "\n",
    "source_directory = \"../../data/NER-Benchmarking/tagger/tagger_output/brat_files/\"\n",
    "\n",
    "create_missing_ann_files(source_directory)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Post processing manual annotations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Check consistency of mentions annotation\n",
    "* Use brat annotation consistency check to create a report of missing annotation and correct those mistakes\n",
    "    * clone BRAT annotation tool repo\n",
    "    * https://github.com/nlplab/brat.git\n",
    "    * use search.py in the brat server to find inconsistencies in the annotation\n",
    "    * Manually correct the annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# consistency check \n",
    "! python /brat/server/src/search.py  -cm  ../../data/NER-Benchmarking/annotations/raw/*.ann >  ../../data/NER-Benchmarking/annotations/raw/missing_mentions.tsv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.Replacing the generic Lifestyle_factor with corresponding attribute"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* By default annotations are categorized by different attributes which are inserted in the *.ann files as followings:\n",
    "    * T1\tLifestyle_factor 56 79\tagricultural wastewater\n",
    "    * A1\tEnvironmental_exposures T1\n",
    "    * T2\tLifestyle_factor 899 922\tagricultural wastewater\n",
    "    * A2\tEnvironmental_exposures T2\n",
    "    * T3\tLifestyle_factor 204 210\tTaiwan\n",
    "    * A3\tGeographical_Feature T3\n",
    "    * T4\tLifestyle_factor 232 245\twater quality\n",
    "    * A4\tEnvironmental_exposures T4\n",
    "    * T5\tLifestyle_factor 704 717\twater quality\n",
    "    * A5\tEnvironmental_exposures T5\n",
    "* But we want to replace Lifestyle_factor with the exact lifestyle branch which make it easy to compute categorized metrics for every branch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "def process_annotation_files(input_directory, output_directory, skip_lsf_out_of_context=False):\n",
    "    # List all input files in the directory\n",
    "    input_files = [f for f in os.listdir(input_directory) if f.endswith('.ann')]\n",
    "    \n",
    "    # Process each .ann input file\n",
    "    for input_file in input_files:\n",
    "        # Initialize an empty DataFrame to keep the updated annotations\n",
    "        updated_annotations = pd.DataFrame(columns=['id', 'lsf_type', 'enity'])\n",
    "\n",
    "        input_file_path = os.path.join(input_directory, input_file)\n",
    "        with open(input_file_path, 'r') as file:\n",
    "            lines = file.readlines()\n",
    "\n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "\n",
    "            if line.startswith('T'):\n",
    "                new_row = line.split('\\t')\n",
    "                new_row_df = pd.DataFrame([new_row], columns=updated_annotations.columns)\n",
    "                updated_annotations = updated_annotations.append(new_row_df, ignore_index=True)\n",
    "            elif line.startswith('A'):\n",
    "                if skip_lsf_out_of_context:\n",
    "                    new_type = line.split()[1]\n",
    "                    if new_type == 'LSF_out_of_context':\n",
    "                        continue\n",
    "                new_type = line.split()[1]\n",
    "                target_id = line.split()[-1]\n",
    "                old_type = updated_annotations.loc[updated_annotations['id'] == target_id, 'lsf_type'].tolist()[0]\n",
    "                old_type = old_type.split()\n",
    "                if old_type[0] != 'LSF_out_of_context':\n",
    "                    old_type[0] = new_type\n",
    "                updated_type = ' '.join(old_type)\n",
    "                updated_annotations.loc[updated_annotations['id'] == target_id, 'lsf_type'] = updated_type\n",
    "\n",
    "        # Write modified content to a new output .ann file\n",
    "        output_file_path = os.path.join(output_directory, input_file)  # Specify the output directory\n",
    "        updated_annotations.to_csv(output_file_path, sep='\\t', index=None, header=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Create different variations of annotations regarding OOC mentions\n",
    "* Annotations have mentions with 'LSF_out_of_context' as attribute which says these mentions are LSF but not in this context\n",
    "* We create two different copies of annotations regaridng 'LSF_out_of_context'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 1. Create a modified vesrion of the annotations where LSF_out_of_context attributes are removed leaving the entity with initial attrubute type \n",
    "    * So as the result OOC entities will apeare as normal LSF type with a types assigned which is determined by attribute type\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "input_directory = '../../data/NER-Benchmarking/annotations/raw/'\n",
    "output_directory = '../../data/NER-Benchmarking/annotations/categorized_no_OOC/'\n",
    "skip_lsf_out_of_context = True  # Set to True to skip lines with 'LSF_out_of_context' as the new type\n",
    "process_annotation_files(input_directory, output_directory, skip_lsf_out_of_context)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 2. Create a modified vesrion of the annotations where LSF_out_of_context attributes are assigned as entity types \n",
    "    * So as the result OOC entities will apeare with LSF_out_of_context as their type\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "input_directory = '../../data/NER-Benchmarking/annotations/raw/'\n",
    "output_directory = '../../data/NER-Benchmarking/annotations/categorized/'\n",
    "skip_lsf_out_of_context = False  # Set to True to skip lines with 'LSF_out_of_context' as the new type\n",
    "process_annotation_files(input_directory, output_directory, skip_lsf_out_of_context)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run benchmark\n",
    "\n",
    "* The 2>&1 at the end of the command ensures that both stdout and stderr are redirected to the full_output.txt file.\n",
    "* all the execustion  excludes Geographical_Feature and Occupations because these are annotated initially for future use cases and it is not intended from NER system to detect Geographical_Feature such as country names or different Occupations, also 'Out-of-scope' is excluded in all analysis since it removes cases such as 'work' in this example: 'In this work ...'\n",
    "\n",
    "    *  -f  Geographical_Feature,Occupations\n",
    "\n",
    "\n",
    "# We run benchmark in two different versions:\n",
    "1. Single LSF type\n",
    "2. Categorized LSF types\n",
    "\n",
    "\n",
    "# 1. Single LSF type\n",
    "\n",
    "### Scenario 1 (published):\n",
    "\n",
    "*  we use -i which ignores the types, because the idea is not to check if we can distinguish correctly between different LSF branches and only annotating LSF factors\n",
    "\n",
    "\n",
    "* Performance:\n",
    "\n",
    "    * precision 96.01% (938/977) recall 49.39% (927/1877) F 65.22%\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Note: (heavy smoker) issue solved (file name: 21719896)\n",
    "* T9\tDrugs 1086 1099\theavy smokers from '../data/200_abstracts/taggers_matches/brat_files/' \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "# Define the file paths as Bash variables\n",
    "echo 'annotation_directory=\"../../data/NER-Benchmarking/annotations/categorized/\"' > variable.sh\n",
    "echo 'tagger_directory=\"../../data/NER-Benchmarking/tagger/tagger_output/brat_files/\"' >> variable.sh\n",
    "echo 'output_file_single_lsf_type_OOC_as_LSF=\"../../data/NER-Benchmarking/benchmark_results/output_single_lsf_type_OOC_as_LSF.txt\"' >> variable.sh\n",
    "\n",
    "echo 'output_file_single_lsf_type_OOC_as_LSF_FP_Lines=\"../../data/NER-Benchmarking/benchmark_results/output_file_single_lsf_type_OOC_as_LSF_FP_Lines.txt\"' >> variable.sh\n",
    "echo 'output_file_single_lsf_type_OOC_as_LSF_FN_Lines=\"../../data/NER-Benchmarking/benchmark_results/output_file_single_lsf_type_OOC_as_LSF_FN_Lines.txt\"' >> variable.sh\n",
    "echo 'output_file_single_lsf_type_OOC_as_LSF_performance_Lines=\"../../data/NER-Benchmarking/benchmark_results/output_file_single_lsf_type_OOC_as_LSF_performance_Lines.txt\"' >> variable.sh\n",
    "\n",
    "source variable.sh\n",
    "\n",
    "# Run your Python script with the variables\n",
    "python2 ../../utils/IAA.py -o -d -v -i -f Out-of-scope,Geographical_Feature,Occupations \"$annotation_directory\" \"$tagger_directory\" --allowmissing > \"$output_file_single_lsf_type_OOC_as_LSF\" 2>&1\n",
    "\n",
    "grep 'FP:' \"$output_file_single_lsf_type_OOC_as_LSF\" > \"$output_file_single_lsf_type_OOC_as_LSF_FP_Lines\"\n",
    "grep 'FN:' \"$output_file_single_lsf_type_OOC_as_LSF\" > \"$output_file_single_lsf_type_OOC_as_LSF_FN_Lines\"\n",
    "grep  -vE 'TP:|TN:|FP:|FN:|MATCH:' \"$output_file_single_lsf_type_OOC_as_LSF\" > \"$output_file_single_lsf_type_OOC_as_LSF_performance_Lines\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scenario 2 (published):\n",
    "* This is used to do error analysis\n",
    "\n",
    "* This is the actual performance of the NER since OOCs (LSF_out_of_context) will be excluded (by -f) \n",
    "* Without excluding LSF_out_of_context they were improving the performance because if we ignore type (which is LSF_out_of_context for them) in case that they are matching tagger result they will improve result\n",
    "\n",
    "*  we use -i which ignores the types, because the idea is not to check if we can distinguish correctly between different LSF branches and only annotating LSF factors\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "# Define the file paths as Bash variables\n",
    "echo 'output_file_single_lsf_type_OOCs_filtered=\"../../data/NER-Benchmarking/benchmark_results/output_single_lsf_type_OOCs_filtered.txt\"' >> variable.sh\n",
    "echo 'output_file_single_lsf_type_OOCs_filtered_FP_Lines=\"../../data/NER-Benchmarking/benchmark_results/output_file_single_lsf_type_OOCs_filtered_FP_Lines.txt\"' >> variable.sh\n",
    "echo 'output_file_single_lsf_type_OOCs_filtered_FN_Lines=\"../../data/NER-Benchmarking/benchmark_results/output_file_single_lsf_type_OOCs_filtered_FN_Lines.txt\"' >> variable.sh\n",
    "\n",
    "source variable.sh\n",
    "\n",
    "# Run your Python script with the variables\n",
    "python2 ../../utils/IAA.py -o -d -v -i -f Out-of-scope,LSF_out_of_context,Geographical_Feature,Occupations \"$annotation_directory\" \"$tagger_directory\" --allowmissing > \"$output_file_single_lsf_type_OOCs_filtered\" 2>&1\n",
    "\n",
    "grep 'FP:' \"$output_file_single_lsf_type_OOCs_filtered\" > \"$output_file_single_lsf_type_OOCs_filtered_FP_Lines\"\n",
    "grep 'FN:' \"$output_file_single_lsf_type_OOCs_filtered\" > \"$output_file_single_lsf_type_OOCs_filtered_FN_Lines\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Different LSF branches (plot) :\n",
    "* run with distinguished LSF branches\n",
    "*  if we remove -i option and run the benchmark it will show also how well is the performance for different LSF branches\n",
    "    * The result now is a little worse because if the annotator was wrong about selecting the correct branch or in general if both Tagger and annotator agree on annotating a term as LSF but they disagree on the branch this will result in a FP or FN which reduces the performance    \n",
    "* The goal is to compare the categorized performance in two different options regarding OOC\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Use a vesrion of annotations where OOCs are apeared as different type ('LSF_out_of_context') in the annotation file so no matter is we filter them out using -f parameter or not they will be ignored since the categorized version reports performance independently for that beside other LSF types and we ignore it in the plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "# Define the file paths as Bash variables\n",
    "echo 'output_file_categorized_lsf_type_OOCs_filtered=\"../../data/NER-Benchmarking/benchmark_results/output_categorized_lsf_type_OOCs_filtered.txt\"' >> variable.sh\n",
    "echo 'output_file_categorized_lsf_type_OOCs_filtered_FP_Lines=\"../../data/NER-Benchmarking/benchmark_results/output_categorized_lsf_type_OOCs_filtered_FP_Lines.txt\"' >> variable.sh\n",
    "echo 'output_file_categorized_lsf_type_OOCs_filtered_FN_Lines=\"../../data/NER-Benchmarking/benchmark_results/output_categorized_lsf_type_OOCs_filtered_FN_Lines.txt\"' >> variable.sh\n",
    "\n",
    "\n",
    "source variable.sh\n",
    "\n",
    "# Run your Python script with the variables\n",
    "python2 ../../utils/IAA.py -o -d -v -f Out-of-scope,LSF_out_of_context,Geographical_Feature,Occupations \"$annotation_directory\" \"$tagger_directory\" --allowmissing > \"$output_file_categorized_lsf_type_OOCs_filtered\" 2>&1\n",
    "\n",
    "grep 'FP:' \"$output_file_categorized_lsf_type_OOCs_filtered\" > \"$output_file_categorized_lsf_type_OOCs_filtered_FP_Lines\"\n",
    "grep 'FN:' \"$output_file_categorized_lsf_type_OOCs_filtered\" > \"$output_file_categorized_lsf_type_OOCs_filtered_FN_Lines\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Use a vesrion of annotations where OOCs are apeared as normal LSF types, and thoses cases are reported along with correspoing LSF branch (this makes it possible to compare treating the OOCs as LSF <case 2.> or removing them <case 1.> how affects the performance of different branches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "# a version of annotations were OOCs are appeares as corresponding LSF type\n",
    "echo 'annotation_no_OOC_directory=\"../../data/NER-Benchmarking/annotations/categorized_no_OOC/\"' >> variable.sh\n",
    "\n",
    "\n",
    "# Define the file paths as Bash variables\n",
    "echo 'output_file_categorized_lsf_type_OOCs_as_LSF=\"../../data/NER-Benchmarking/benchmark_results/output_categorized_lsf_type_OOCs_as_LSF.txt\"' >> variable.sh\n",
    "echo 'output_file_categorized_lsf_type_OOCs_as_LSF_FP_Lines=\"../../data/NER-Benchmarking/benchmark_results/output_categorized_lsf_type_OOCs_as_LSF_FP_Lines.txt\"' >> variable.sh\n",
    "echo 'output_file_categorized_lsf_type_OOCs_as_LSF_FN_Lines=\"../../data/NER-Benchmarking/benchmark_results/output_categorized_lsf_type_OOCs_as_LSF_FN_Lines.txt\"' >> variable.sh\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "source variable.sh\n",
    "\n",
    "# Run your Python script with the variables\n",
    "python2 ../../utils/IAA.py -o -d -v -f Out-of-scope,Geographical_Feature,Occupations \"$annotation_no_OOC_directory\" \"$tagger_directory\" --allowmissing > \"$output_file_categorized_lsf_type_OOCs_as_LSF\" 2>&1\n",
    "\n",
    "grep 'FP:' \"$output_file_categorized_lsf_type_OOCs_as_LSF\" > \"$output_file_categorized_lsf_type_OOCs_as_LSF_FP_Lines\"\n",
    "grep 'FN:' \"$output_file_categorized_lsf_type_OOCs_as_LSF\" > \"$output_file_categorized_lsf_type_OOCs_as_LSF_FN_Lines\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* extract metrics from text files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def extract_data_and_save(file_path):\n",
    "    # Initialize an empty DataFrame\n",
    "    pr_rec_per_lsf_branch = pd.DataFrame(columns=['Lifestyle-factor branch', 'Precision', 'Recall', 'F'])\n",
    "\n",
    "    # Read the text from the file\n",
    "    with open(file_path, 'r') as file:\n",
    "        text = file.read()\n",
    "\n",
    "    # Split the text into lines\n",
    "    lines = text.strip().split('\\n')\n",
    "\n",
    "    # Iterate over each line and extract data\n",
    "    for line in lines:\n",
    "        match = re.match(r'TYPE:\\s+(.*?)\\s+precision\\s+([\\d.]+%)\\s+\\((\\d+)/(\\d+)\\)\\s+recall\\s+([\\d.]+%)\\s+\\((\\d+)/(\\d+)\\)\\s+F\\s+([\\d.]+)', line)\n",
    "        if match:\n",
    "            revision_step, precision, _, _, recall, _, _, f_score = match.groups()\n",
    "            if revision_step not in ['Beauty_and_Cleaning', 'Drugs', 'Environmental_exposures',\n",
    "                                     'Mental_health_practices', 'Non_physical_leisure_time_activities',\n",
    "                                     'Nutrition', 'Physical_activity', 'Sleep', 'Socioeconomic_factors']:\n",
    "                continue\n",
    "\n",
    "            # Remove the percentage symbols and convert to floats\n",
    "            precision = float(precision.rstrip('%'))\n",
    "            recall = float(recall.rstrip('%'))\n",
    "            f_score = float(f_score)\n",
    "            # Append the data to the DataFrame\n",
    "            pr_rec_per_lsf_branch = pr_rec_per_lsf_branch.append({\n",
    "                'Lifestyle-factor branch': revision_step.strip(),\n",
    "                'Precision': precision,\n",
    "                'Recall': recall,\n",
    "                'F': f_score\n",
    "            }, ignore_index=True)\n",
    "\n",
    "    # Sort the DataFrame by the 'F' column in ascending order\n",
    "    pr_rec_per_lsf_branch = pr_rec_per_lsf_branch.sort_values(by='F')\n",
    "    \n",
    "    # Return the resulting DataFrame\n",
    "    return pr_rec_per_lsf_branch\n",
    "\n",
    "# full_output_categorized_exclude_OOC:\n",
    "file_path = \"../../data/NER-Benchmarking/benchmark_results/output_categorized_lsf_type_OOCs_filtered.txt\"\n",
    "resulting_df = extract_data_and_save(file_path)\n",
    "# Save the DataFrame to a TSV file\n",
    "tsv_file_path =\"../../data/NER-Benchmarking/benchmark_results/output_categorized_lsf_type_OOCs_filtered_prec_rec.tsv\"\n",
    "resulting_df.to_csv(tsv_file_path, sep='\\t', index=None)\n",
    "\n",
    "\n",
    "\n",
    "# full_output_catgeorized:\n",
    "file_path = \"../../data/NER-Benchmarking/benchmark_results/output_categorized_lsf_type_OOCs_as_LSF.txt\"\n",
    "resulting_df = extract_data_and_save(file_path)\n",
    "# Save the DataFrame to a TSV file\n",
    "tsv_file_path = \"../../data/NER-Benchmarking/benchmark_results/output_categorized_lsf_type_OOCs_as_LSF_prec_rec.tsv\"\n",
    "resulting_df.to_csv(tsv_file_path, sep='\\t', index=None)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Modify lables to be appropriate for plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def replace_phrases_in_dataframe(input_file_path, phrase_replacements):\n",
    "    # Read the input TSV file into a DataFrame\n",
    "    df = pd.read_csv(input_file_path, sep='\\t')\n",
    "\n",
    "    # Replace the phrases in the DataFrame\n",
    "    df['Lifestyle-factor branch'] = df['Lifestyle-factor branch'].replace(phrase_replacements)\n",
    "\n",
    "    # Write the modified DataFrame to the output TSV file\n",
    "    df.to_csv(input_file_path, sep='\\t', index=False)\n",
    "\n",
    "\n",
    "phrase_replacements = {\n",
    "    'Environmental_exposures': 'Environmental exposures',\n",
    "    'Physical_activity': 'Physical activities',\n",
    "    'Socioeconomic_factors': 'Socioeconomic factors',\n",
    "    'Drugs': 'Substance use',\n",
    "    'Mental_health_practices': 'Mental health practices',\n",
    "    'Non_physical_leisure_time_activities': 'Non physical leisure time activities',\n",
    "    'Beauty_and_Cleaning': 'Beauty and cleaning'\n",
    "}\n",
    "\n",
    "\n",
    "# replace names in both files\n",
    "input_file_path = \"../../data/NER-Benchmarking/benchmark_results/output_categorized_lsf_type_OOCs_filtered_prec_rec.tsv\"\n",
    "replace_phrases_in_dataframe(input_file_path, phrase_replacements)\n",
    "\n",
    "input_file_path = \"../../data/NER-Benchmarking/benchmark_results/output_categorized_lsf_type_OOCs_as_LSF_prec_rec.tsv\"\n",
    "replace_phrases_in_dataframe(input_file_path, phrase_replacements)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge two files\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Load the first TSV file into a DataFrame\n",
    "file1 = \"../../data/NER-Benchmarking/benchmark_results/output_categorized_lsf_type_OOCs_filtered_prec_rec.tsv\"\n",
    "df1 = pd.read_csv(file1, sep='\\t')\n",
    "\n",
    "# Load the second TSV file into a DataFrame, excluding the first column\n",
    "file2 = \"../../data/NER-Benchmarking/benchmark_results/output_categorized_lsf_type_OOCs_as_LSF_prec_rec.tsv\"\n",
    "df2 = pd.read_csv(file2, sep='\\t')\n",
    "\n",
    "# Add \"_ooc\" suffix to column names of the second DataFrame\n",
    "df2.columns = ['Lifestyle-factor branch']+[col + '_ooc_as_LSF' for col in df2.columns if col!='Lifestyle-factor branch']\n",
    "\n",
    "# Merge the two DataFrames based on the \"Lifestyle-factor branch\" column\n",
    "merged_df = df1.merge(df2, on='Lifestyle-factor branch')\n",
    "\n",
    "# Save the merged DataFrame to a new TSV file\n",
    "merged_file = \"../../data/NER-Benchmarking/benchmark_results/plot_input_dict_based.tsv\"\n",
    "\n",
    "\n",
    "#Define the specific order you want for 'Lifestyle-factor branch'\n",
    "desired_order = ['nutrition', 'socioeconomic factors', 'environmental exposures', 'substance use','physical activities', 'beauty and cleaning',  'non physical leisure time activities',  'sleep' , 'mental health practices']\n",
    "# Create a new column with the order of 'Lifestyle-factor branch' (ignoring case)\n",
    "merged_df['Order'] = merged_df['Lifestyle-factor branch'].str.lower().map({value.lower(): index for index, value in enumerate(desired_order)})\n",
    "\n",
    "# Sort the DataFrame based on the new 'Order' column\n",
    "merged_df_sorted = merged_df.sort_values(by='Order').drop('Order', axis=1)\n",
    "\n",
    "\n",
    "merged_df_sorted.to_csv(merged_file, sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. plot using only by excluding OOC cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/dzq660/miniforge3/envs/lsf/lib/python3.8/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "Figure(1400x1200)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "!python3 ./scripts/plot_prec_rec_progression.py --input_file=../../data/NER-Benchmarking/benchmark_results/output_categorized_lsf_type_OOCs_filtered_prec_rec.tsv --task=\"Precision-Recall Plot for Lifestyle-factors NER\" --output_file=../../plots/NER_Benchmark_Tagger.png\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. plot by comparing with and without OOC cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/dzq660/miniforge3/envs/lsf/lib/python3.8/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "Figure(1200x1200)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "!python3  ./scripts/plot_prec_rec_ooc.py  --input_file=../../data/NER-Benchmarking/benchmark_results/plot_input_dict_based.tsv --task=\"\" --output_file=../../plots/NER_Benchmark_Tagger_OOC.png\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
