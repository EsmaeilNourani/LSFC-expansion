{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmarking Transformer-based NER "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cd ../../\n",
    "pwd\n",
    "git submodule add https://github.com/EsmaeilNourani/S1000-transformer-ner.git   S1000_Transformer_NER\n",
    "git add .\n",
    "git commit -m \"S1000-transformer-ner as a submodule in S1000_Transformer_NER directory.\"\n",
    "git push\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cd ../../\n",
    "pwd\n",
    "git submodule add https://github.com/EsmaeilNourani/Lifestyle-factors-classification.git   LSFC\n",
    "git add .\n",
    "git commit -m \"LSFC as a submodule in LSFC directory.\"\n",
    "git push\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cd ../../\n",
    "pwd\n",
    "git submodule add https://github.com/jouniluoma/S1000-transformer-tagger.git   S1000_Transformer_Tagger\n",
    "git add .\n",
    "git commit -m \"S1000-transformer-tagger as a submodule in S1000_Transformer_Tagger directory.\"\n",
    "git push\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cd ../../\n",
    "pwd\n",
    "git submodule add https://github.com/spyysalo/standoff2conll.git   standoff2conll\n",
    "git add .\n",
    "git commit -m \"standoff2conll as a submodule in standoff2conll directory.\"\n",
    "git push\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter annotation files\n",
    "* create a copy of annotation files by excluding: \n",
    "        * keywords_to_remove = {\"Out-of-scope\",\"LSF_out_of_context\", \"Geographical_Feature\", \"Occupations\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files with keywords removed have been created in the output folder.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Input folder containing .ann files\n",
    "input_folder = '../../data/NER-Benchmarking/annotations/categorized/'\n",
    "# Output folder for modified files\n",
    "output_folder = '../../data/NER-Benchmarking/annotations/filtered/'\n",
    "\n",
    "# Keywords to remove\n",
    "keywords_to_remove = {\"Out-of-scope\",\"LSF_out_of_context\", \"Geographical_Feature\", \"Occupations\"}\n",
    "\n",
    "def remove_keywords(input_filepath, output_filepath):\n",
    "    with open(input_filepath, 'r') as input_file, open(output_filepath, 'w') as output_file:\n",
    "        for line in input_file:\n",
    "            line = line.strip()\n",
    "            if not any(keyword in line for keyword in keywords_to_remove):\n",
    "                output_file.write(line + '\\n')\n",
    "\n",
    "# Create the output folder if it doesn't exist\n",
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)\n",
    "\n",
    "# Iterate through .ann files in the input folder\n",
    "for filename in os.listdir(input_folder):\n",
    "    if filename.endswith(\".ann\"):\n",
    "        input_filepath = os.path.join(input_folder, filename)\n",
    "        output_filepath = os.path.join(output_folder, filename)\n",
    "        remove_keywords(input_filepath, output_filepath)\n",
    "\n",
    "print(\"Files with keywords removed have been created in the output folder.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare dataset\n",
    "* 20% hold out test data\n",
    "* 80% divided into 5 fold for cross validation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Split with considering the LSF types distribution between train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribution of Keywords in Test Set:\n",
      "  Mental_health_practices: 11 files\n",
      "  Non_physical_leisure_time_activities: 8 files\n",
      "  Beauty_and_Cleaning: 26 files\n",
      "  Drugs: 34 files\n",
      "  Environmental_exposures: 25 files\n",
      "  Socioeconomic_factors: 52 files\n",
      "  Physical_activity: 21 files\n",
      "  Nutrition: 66 files\n",
      "  Sleep: 20 files\n",
      "\n",
      "Distribution of Keywords in Train Set:\n",
      "  Mental_health_practices: 63 files\n",
      "  Non_physical_leisure_time_activities: 53 files\n",
      "  Beauty_and_Cleaning: 30 files\n",
      "  Drugs: 153 files\n",
      "  Environmental_exposures: 174 files\n",
      "  Socioeconomic_factors: 223 files\n",
      "  Physical_activity: 201 files\n",
      "  Nutrition: 379 files\n",
      "  Sleep: 90 files\n",
      "\n",
      "Number of Keywords in Test Set:\n",
      "Total: 263 files\n",
      "\n",
      "Number of Keywords in Train Set:\n",
      "Total: 1366 files\n",
      "Folding complete.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import random\n",
    "import random\n",
    "\n",
    "# Set the random seed for reproducibility\n",
    "random.seed(1361)\n",
    "# Source directory containing your pairs of .txt and .ann files\n",
    "source_directory_txt = '../../data/NER-Benchmarking/annotations/raw/'\n",
    "source_directory_ann = '../../data/NER-Benchmarking/annotations/filtered/'\n",
    "\n",
    "# Root destination directory for folds and test data\n",
    "root_directory = '../../data/NER-Benchmarking/annotations/balanced_LSF_types_splits/'\n",
    "\n",
    "# Create the root directory if it doesn't exist\n",
    "os.makedirs(root_directory, exist_ok=True)\n",
    "\n",
    "# Create a directory for the test set\n",
    "test_directory = os.path.join(root_directory, 'test_data')\n",
    "os.makedirs(test_directory, exist_ok=True)\n",
    "\n",
    "# List all .txt files in the source directory\n",
    "txt_files = [filename for filename in os.listdir(source_directory_txt) if filename.endswith(\".txt\")]\n",
    "\n",
    "# Shuffle the list of .txt files randomly\n",
    "random.shuffle(txt_files)\n",
    "\n",
    "# Define the 9 keywords\n",
    "keywords = {\n",
    "    'Environmental_exposures',\n",
    "    'Physical_activity',\n",
    "    'Socioeconomic_factors',\n",
    "    'Drugs',\n",
    "    'Mental_health_practices',\n",
    "    'Non_physical_leisure_time_activities',\n",
    "    'Beauty_and_Cleaning',\n",
    "    'Nutrition',\n",
    "    'Sleep'\n",
    "}\n",
    "\n",
    "\n",
    "# List all .txt files in the source directory\n",
    "txt_files = [filename for filename in os.listdir(source_directory_txt) if filename.endswith(\".txt\")]\n",
    "\n",
    "# Shuffle the list of .txt files randomly while keeping the test size at 20%\n",
    "test_size = int(0.20 * len(txt_files))\n",
    "test_files = random.sample(txt_files, test_size)\n",
    "train_files = [file for file in txt_files if file not in test_files]\n",
    "\n",
    "\n",
    "\n",
    "# Create a dictionary to track keyword distribution in each fold\n",
    "keyword_counts = {keyword: [0] * 5 for keyword in keywords}\n",
    "\n",
    "# Calculate the distribution of keywords in the test set\n",
    "keyword_counts_test = {keyword: 0 for keyword in keywords}\n",
    "\n",
    "for txt_file in test_files:\n",
    "    ann_file = txt_file.replace(\".txt\", \".ann\")\n",
    "    source_ann_path = os.path.join(source_directory_ann, ann_file)\n",
    "\n",
    "    # Read the .ann file to identify the keywords\n",
    "    with open(source_ann_path, 'r') as ann_file_content:\n",
    "        for line in ann_file_content:\n",
    "            fields = line.strip().split()\n",
    "            if fields and fields[0].startswith('T'):\n",
    "                keyword = fields[1]  # Assumes the keyword appears as the second field\n",
    "                if keyword in keywords:\n",
    "                    keyword_counts_test[keyword] += 1\n",
    "\n",
    "# Calculate the distribution of keywords in the train set\n",
    "keyword_counts_train = {keyword: 0 for keyword in keywords}\n",
    "\n",
    "for txt_file in train_files:\n",
    "    ann_file = txt_file.replace(\".txt\", \".ann\")\n",
    "    source_ann_path = os.path.join(source_directory_ann, ann_file)\n",
    "\n",
    "    # Read the .ann file to identify the keywords\n",
    "    with open(source_ann_path, 'r') as ann_file_content:\n",
    "        for line in ann_file_content:\n",
    "            fields = line.strip().split()\n",
    "            if fields and fields[0].startswith('T'):\n",
    "                keyword = fields[1]  # Assumes the keyword appears as the second field\n",
    "                if keyword in keywords:\n",
    "                    keyword_counts_train[keyword] += 1\n",
    "\n",
    "# Check keyword distribution in test and train sets\n",
    "print(\"Distribution of Keywords in Test Set:\")\n",
    "for keyword, count in keyword_counts_test.items():\n",
    "    print(f\"  {keyword}: {count} files\")\n",
    "\n",
    "print(\"\\nDistribution of Keywords in Train Set:\")\n",
    "for keyword, count in keyword_counts_train.items():\n",
    "    print(f\"  {keyword}: {count} files\")\n",
    "\n",
    "# Copy test files to the test folder\n",
    "for txt_file in test_files:\n",
    "    ann_file = txt_file.replace(\".txt\", \".ann\")\n",
    "    dest_txt_path = os.path.join(test_directory, txt_file)\n",
    "    dest_ann_path = os.path.join(test_directory, ann_file)\n",
    "    shutil.copy(os.path.join(source_directory_txt, txt_file), dest_txt_path)\n",
    "    shutil.copy(os.path.join(source_directory_ann, ann_file), dest_ann_path)\n",
    "\n",
    "# Create directories for each fold in the train set\n",
    "num_folds = 5\n",
    "fold_size = len(train_files) // num_folds\n",
    "\n",
    "for fold in range(num_folds):\n",
    "    fold_directory = os.path.join(root_directory, f'fold_{fold + 1}')\n",
    "    os.makedirs(fold_directory, exist_ok=True)\n",
    "\n",
    "# Iterate over the .txt files in the train set and distribute them to folds\n",
    "for txt_file in train_files:\n",
    "    ann_file = txt_file.replace(\".txt\", \".ann\")\n",
    "    source_ann_path = os.path.join(source_directory_ann, ann_file)\n",
    "\n",
    "    fold_idx = txt_files.index(txt_file) % num_folds\n",
    "    fold_directory = os.path.join(root_directory, f'fold_{fold_idx + 1}')\n",
    "    dest_txt_path = os.path.join(fold_directory, txt_file)\n",
    "    dest_ann_path = os.path.join(fold_directory, ann_file)\n",
    "\n",
    "    shutil.copy(os.path.join(source_directory_txt, txt_file), dest_txt_path)\n",
    "    shutil.copy(source_ann_path, dest_ann_path)\n",
    "\n",
    "\n",
    "print(\"\\nNumber of Keywords in Test Set:\")\n",
    "total_keywords_test = sum(keyword_counts_test.values())\n",
    "\n",
    "print(f\"Total: {total_keywords_test} files\")\n",
    "\n",
    "print(\"\\nNumber of Keywords in Train Set:\")\n",
    "total_keywords_train = sum(keyword_counts_train.values())\n",
    "print(f\"Total: {total_keywords_train} files\")\n",
    "\n",
    "print(\"Folding complete.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert to CONLL format\n",
    "* Note: this can be done even before spliiting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "\n",
    "def convert_to_CONLL(root_folder, script_path, output_suffix, singletype=None):\n",
    "    for subdir in os.listdir(root_folder):\n",
    "        subdir_path = os.path.join(root_folder, subdir)\n",
    "        if os.path.isdir(subdir_path):\n",
    "            output_file = os.path.join(root_folder, f'{subdir}_{output_suffix}.tsv')\n",
    "            if singletype:\n",
    "                command = f\"python  {script_path}  -1 {singletype} {subdir_path} > {output_file}\"\n",
    "                subprocess.call(command, shell=True)\n",
    "            else:\n",
    "                command = f\"python  {script_path}  {subdir_path} > {output_file}\"\n",
    "                subprocess.call(command, shell=True)\n",
    "\n",
    "# Example usage:\n",
    "root_folder = '../../data/NER-Benchmarking/annotations/balanced_LSF_types_splits/'\n",
    "script_path = './standoff2conll/standoff2conll.py'\n",
    "output_suffix = 'merged_only_LSF'  # Customize the output file suffix\n",
    "\n",
    "# Call the function with -1 LSF in arguments\n",
    "singletype = 'LSF'  # Replace with the desired singletype\n",
    "convert_to_CONLL(root_folder, script_path, output_suffix, singletype)\n",
    "\n",
    "# Call the function without -1 LSF in arguments\n",
    "# removed -1 LSF to explicitly mention LSF branch instead of single LSF type\n",
    "output_suffix='merged_with_LSF_branches'\n",
    "convert_to_CONLL(root_folder, script_path, output_suffix)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge folds to create single train file\n",
    "    * to be used after grid search for training the final model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def concatenate_files(file_names, output_file):\n",
    "    # Initialize an empty DataFrame to store the concatenated data\n",
    "    concatenated_df = pd.DataFrame()\n",
    "\n",
    "    # Loop through each file and concatenate its contents to the DataFrame\n",
    "    for file_name in file_names:\n",
    "        # Assuming that the first row contains column headers\n",
    "        df = pd.read_csv(file_name, sep='\\t', header=None)\n",
    "        concatenated_df = pd.concat([concatenated_df, df], ignore_index=True)\n",
    "\n",
    "    # Save the concatenated DataFrame to a new TSV file\n",
    "    concatenated_df.to_csv(output_file, sep='\\t', index=False, header=None)\n",
    "\n",
    "\n",
    "\n",
    "# List of file names to concatenate\n",
    "file_names = [\"../../data/s1000_ner/balanced_LSF_types_splits/fold_1_merged_only_LSF.tsv\",\n",
    "              \"../../data/s1000_ner/balanced_LSF_types_splits/fold_2_merged_only_LSF.tsv\",\n",
    "              \"../../data/s1000_ner/balanced_LSF_types_splits/fold_3_merged_only_LSF.tsv\",\n",
    "              \"../../data/s1000_ner/balanced_LSF_types_splits/fold_4_merged_only_LSF.tsv\",\n",
    "              \"../../data/s1000_ner/balanced_LSF_types_splits/fold_5_merged_only_LSF.tsv\"]\n",
    "              \n",
    "\n",
    "# Output file name\n",
    "output_file = \"../../data/s1000_ner/balanced_LSF_types_splits/train_data_merged_only_LSF.tsv\"\n",
    "\n",
    "# Call the function to concatenate files\n",
    "concatenate_files(file_names, output_file)\n",
    "\n",
    "\n",
    "\n",
    "# List of file names to concatenate\n",
    "file_names = ['../../data/s1000_ner/balanced_LSF_types_splits/fold_1_merged_with_LSF_branches.tsv',\n",
    "              '../../data/s1000_ner/balanced_LSF_types_splits/fold_2_merged_with_LSF_branches.tsv',\n",
    "              '../../data/s1000_ner/balanced_LSF_types_splits/fold_3_merged_with_LSF_branches.tsv',\n",
    "              '../../data/s1000_ner/balanced_LSF_types_splits/fold_4_merged_with_LSF_branches.tsv',\n",
    "              '../../data/s1000_ner/balanced_LSF_types_splits/fold_5_merged_with_LSF_branches.tsv']\n",
    "\n",
    "# Output file name\n",
    "output_file = \"../../data/s1000_ner/balanced_LSF_types_splits/train_data_merged_with_LSF_branches.tsv\"\n",
    "\n",
    "# Call the function to concatenate files\n",
    "concatenate_files(file_names, output_file)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer data to computerome"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* data dir on computerome:\n",
    "    * /home/projects/ku_10024/scratch/esmaeil/s1000/S1000-transformer-ner/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* script to run NER on computerome:\n",
    "    * /home/projects/ku_10024/scratch/esmaeil/s1000/S1000-transformer-ner/scripts/run-ner.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Documentation of Grid Search "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* To find best hyperparameters we use the second approach from the following options:\n",
    "\n",
    "In cross-validation, calculating precision and recall by summing TP (True Positives), FP (False Positives), and FN (False Negatives) across all folds can yield different results compared to averaging precision and recall across folds. Here's how they differ:\n",
    "\n",
    "1. Averaging Precision and Recall Across Folds:\n",
    "   - For each fold, you calculate precision and recall separately.\n",
    "   - Then, you average the precision and recall values obtained from all folds.\n",
    "   - This method gives equal weight to each fold, ensuring that the contribution of each fold to the final result is the same.\n",
    "\n",
    "2. Summing TP, FP, and FN Across All Folds:\n",
    "   - In this approach, you sum TP, FP, and FN across all folds to calculate the overall values for the entire dataset.\n",
    "   - Afterward, you calculate precision and recall based on these summed values.\n",
    "   - This method treats the entire dataset as a single unit and computes precision and recall as if it were a single large dataset.\n",
    "\n",
    "   Here's how you can calculate precision and recall in the summing approach for each fold:\n",
    "\n",
    "    Sum TP, FP, and FN across all folds:\n",
    "        Calculate the total TP, FP, and FN across all folds by summing the corresponding counts from each fold.\n",
    "\n",
    "    Calculate Precision and Recall:\n",
    "\n",
    "        Once you have the total TP, FP, and FN counts, you can calculate precision and recall as follows:\n",
    "\n",
    "        Precision = Total TP / (Total TP + Total FP)\n",
    "\n",
    "        Recall = Total TP / (Total TP + Total FN)\n",
    "\n",
    "The key difference lies in how they handle the division of the dataset. Here are some considerations for each approach:\n",
    "\n",
    "- Averaging precision and recall across folds is useful when you want to evaluate the model's performance on each fold independently and then obtain an average performance metric. It provides insights into how well the model generalizes across different subsets of the data.\n",
    "\n",
    "- Summing TP, FP, and FN across all folds treats the entire dataset as a whole, which can be beneficial when you want an overall assessment of the model's performance across the entire dataset. However, it may not provide information about how consistent the model's performance is across different subsets of data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update scripts\n",
    "* update ner_hf_trainer.py  (/home/projects/ku_10024/scratch/esmaeil/s1000/S1000-transformer-ner)\n",
    "    * we replaced this \n",
    "    * results.append([conlleval.metrics(c)[0].prec, conlleval.metrics(c)[0].rec, conlleval.metrics(c)[0].fscore]) \n",
    "    * with \n",
    "    * results.append([conlleval.metrics(c)[0].prec, conlleval.metrics(c)[0].rec, conlleval.metrics(c)[0].fscore,conlleval.metrics(c)[0].tp,conlleval.metrics(c)[0].fp,conlleval.metrics(c)[0].fn])\n",
    "    * to also report tp,fp,fn to be able to calculate Total metrics\n",
    "    * Note: [0] in these lines shows the metric calculate for overal types and not specific entity type (here we have only one type: LSF)\n",
    "        * overal metric is calculated by Report funstion in  conlleval.py where it returns   overall, by_type = metrics(counts)\n",
    "        * so [0] refers to overal\n",
    " \n",
    "    * Note: also in ner_hf_trainer.py 3 different evaluation type is introduced and reported in the result file:\n",
    "        * method_names = ['CMV','CMVP','F']\n",
    "        * for each type a line as appended in the result file, so for each fold (in cross validation) there will be three lines in the result file\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computerome"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary:\n",
    "1. Grid Search\n",
    "    * qsub  qsub_ner_grid.sh to do grid search\n",
    "    * raw results will be saved to /home/projects/ku_10024/scratch/esmaeil/s1000/S1000-transformer-ner/results/results-new-grid.csv\n",
    "    * then we summ TP,FP, Fn and calculate overall results and save it in local dir: '../../data/s1000_ner/results/results-new-grid-summed.csv'\n",
    "2. Run with final hyperparametrs from grid search\n",
    "    * merge all 5 folds to create a merged train file: /home/projects/ku_10024/scratch/esmaeil/s1000/S1000-transformer-ner/data/train_data_merged.tsv\n",
    "    * test data: /home/projects/ku_10024/scratch/esmaeil/s1000/S1000-transformer-ner/data/test_data_merged.tsv\n",
    "    * qsub qsub_ner_train_test.sh\n",
    "    * results will in /home/projects/ku_10024/scratch/esmaeil/s1000/S1000-transformer-ner/output/output_final_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LLM Models:\n",
    "* RoBERTa large path on computerome\n",
    "    * /home/projects/ku_10024/scratch/esmaeil/s1000/S1000-transformer-ner/RoBERTa-large-PM-M3-Voc/RoBERTa-large-PM-M3-Voc-hf\n",
    "* Finetuned model path:\n",
    "    * /home/projects/ku_10024/scratch/esmaeil/s1000/S1000-transformer-ner/S1000-transformer-ner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* iqsub with gpu\n",
    "\n",
    "* Activate env\n",
    "    * module load tools\n",
    "    * cd /home/projects/ku_10024/scratch/esmaeil/s1000\n",
    "    * conda  activate /home/projects/ku_10024/scratch/esmaeil/s1000/s1000\n",
    "        * or source activate /home/projects/ku_10024/scratch/esmaeil/s1000/s1000\n",
    "    * cd S1000-transformer-ner/\n",
    "    * \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modified Files and Scripts:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### result file:\n",
    "* I chnaged this line in ner_hf_trainer.py\n",
    "\n",
    "    * with open(result_file, 'w+') as f:\n",
    "    * to \n",
    "    * with open(result_file, 'a') as f:\n",
    "    * to append results for different folds and also different set of hyperparameters\n",
    "\n",
    "* /home/projects/ku_10024/scratch/esmaeil/s1000/S1000-transformer-ner/results/results-output.csv\n",
    "* each row contains:\n",
    "    * args.output_file,\n",
    "    * args.max_seq_length, \n",
    "    * args.model_name, \n",
    "    * args.num_train_epochs, \n",
    "    * args.learning_rate,\n",
    "    * args.batch_size,\n",
    "    * args.predict_position,\n",
    "    * args.train_data,\n",
    "    * args.test_data,\n",
    "    * method_name (one of ['CMV','CMVP','F']) so there will three lines\n",
    "    * prec\n",
    "    * rec\n",
    "    * fscore\n",
    "    * tp\n",
    "    * fp\n",
    "    * fn\n",
    "    * example: output\t128\tRoBERTa-large-PM-M3-Voc/RoBERTa-large-PM-M3-Voc-hf\t10.0\t3E-05\t4\t0\tdata/train.tsv\tdata/dev.tsv\tF\t0.5689655172413793\t0.5689655172413793\t0.5689655172413793\t66\t50\t50\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modified /scripts/run-ner.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* There are three different versions of this script in this path:\n",
    "    * /home/projects/ku_10024/scratch/esmaeil/s1000/S1000-transformer-ner/scripts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. For single run: run-ner-single.sh\n",
    "    * ./scripts/run-ner-single.sh\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. For folds: run-ner-folds.sh\n",
    "    * ./scripts/run-ner-folds.sh\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. For grid search: run-ner-grid.sh\n",
    "    * normally it will be executed using qsub: qsub run-ner-grid.sh\n",
    "    * checkjob -v  46045344 to see the dedicated resources\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output files:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 1. /home/projects/ku_10024/scratch/esmaeil/s1000/S1000-transformer-ner/output/ \n",
    "    * contain three files for actuall NER output whcih shows what entities are detected in th BIO format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 2. /home/projects/ku_10024/scratch/esmaeil/s1000/S1000-transformer-ner/results\n",
    "    * contains results-output.csv file which has all performance results along with TP,FP,FN counts for different experiments\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process results from computerome"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Possible explanations for different metrics provided in the result files:\n",
    "    * method_names = ['CMV','CMVP','F']\n",
    "\n",
    "    * 'F':  'F' likely stands for \"First\" or \"First Prediction.\" This means that the 'F' method represents a strategy where the NER label for each token is determined based on the label from the first prediction (the initial prediction) for that token. In contrast to the other methods that involve voting or accumulating probabilities, this strategy simply takes the label from the first prediction as the final label for each token.\n",
    "    * 'CMV': This likely stands for \"Consensus Most Voted\" or a similar term. It corresponds to the strategy where the NER label for each token is determined by selecting the label that occurs most frequently among all the model's predictions for that token. This method is represented by the pr_ensemble variable and is based on the \"First tag then vote\" strategy mentioned in your comments.\n",
    "    * 'CMVP': This likely stands for \"Consensus Most Voted with Probabilities\" or something similar. It corresponds to the strategy where the NER label for each token is determined by selecting the label with the highest accumulated probability among all the model's predicted probabilities for that token. This method is represented by the prob_ensemble variable and is based on the \"Accumulate probabilities, then vote\" strategy mentioned in your comments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* check job status:\n",
    "    * checkjob -v 46215868\n",
    "    * qstat -f 46215868\n",
    "* kill job \n",
    "    * mjobctl -c <jobid> or canceljob <jobid>\n",
    "    * Force cancel job, try this if regular cancel fails\n",
    "    * mjobctl -F <jobid>\n",
    "    mjobctl -F  46221248\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### ______________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Grid Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### ______________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 1. ssh C2\n",
    "* 2. cd /home/projects/ku_10024/scratch/esmaeil/s1000/S1000-transformer-ner/scripts\n",
    "* 3. qsub qsub_ner_grid_with_LSF_branches.sh\n",
    "* 4. output file is copied from computerome(/home/projects/ku_10024/scratch/esmaeil/s1000/S1000-transformer-ner/results) to '../../data/s1000_ner/results/results-new-grid.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "#results=pd.read_csv('../../data/s1000_ner/results/results-output.csv',sep='\\t')\n",
    "results=pd.read_csv('../../data/s1000_ner/results/results-new-grid.csv')\n",
    "results.columns=['experiment_ID','max_seq_length','model_name','num_train_epochs','learning_rate','batch_size','predict_position','train_data','test_data','method','prec','rec','F','TP','FP','FN']\n",
    "# # remove old results from the file\n",
    "# # Drop the first n rows\n",
    "# results = results.drop(results.index[:182])\n",
    "# # Reset the index if needed\n",
    "# results = results.reset_index(drop=True)\n",
    "\n",
    "# results.to_csv('../../data/s1000_ner/results/results-output.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(326, 16)"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>experiment_ID</th>\n",
       "      <th>max_seq_length</th>\n",
       "      <th>model_name</th>\n",
       "      <th>num_train_epochs</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>predict_position</th>\n",
       "      <th>train_data</th>\n",
       "      <th>test_data</th>\n",
       "      <th>method</th>\n",
       "      <th>prec</th>\n",
       "      <th>rec</th>\n",
       "      <th>F</th>\n",
       "      <th>TP</th>\n",
       "      <th>FP</th>\n",
       "      <th>FN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>fold_1_epoch_20_lr1e-5_batch16_seq128</td>\n",
       "      <td>128</td>\n",
       "      <td>RoBERTa-large-PM-M3-Voc/RoBERTa-large-PM-M3-Vo...</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>/home/projects/ku_10024/scratch/esmaeil/s1000/...</td>\n",
       "      <td>/home/projects/ku_10024/scratch/esmaeil/s1000/...</td>\n",
       "      <td>CMVP</td>\n",
       "      <td>0.610390</td>\n",
       "      <td>0.698020</td>\n",
       "      <td>0.651270</td>\n",
       "      <td>141</td>\n",
       "      <td>90</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>fold_1_epoch_20_lr1e-5_batch16_seq128</td>\n",
       "      <td>128</td>\n",
       "      <td>RoBERTa-large-PM-M3-Voc/RoBERTa-large-PM-M3-Vo...</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>/home/projects/ku_10024/scratch/esmaeil/s1000/...</td>\n",
       "      <td>/home/projects/ku_10024/scratch/esmaeil/s1000/...</td>\n",
       "      <td>F</td>\n",
       "      <td>0.601732</td>\n",
       "      <td>0.688119</td>\n",
       "      <td>0.642032</td>\n",
       "      <td>139</td>\n",
       "      <td>92</td>\n",
       "      <td>63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>fold_2_epoch_20_lr1e-5_batch16_seq128</td>\n",
       "      <td>128</td>\n",
       "      <td>RoBERTa-large-PM-M3-Voc/RoBERTa-large-PM-M3-Vo...</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>/home/projects/ku_10024/scratch/esmaeil/s1000/...</td>\n",
       "      <td>/home/projects/ku_10024/scratch/esmaeil/s1000/...</td>\n",
       "      <td>CMV</td>\n",
       "      <td>0.663185</td>\n",
       "      <td>0.709497</td>\n",
       "      <td>0.685560</td>\n",
       "      <td>254</td>\n",
       "      <td>129</td>\n",
       "      <td>104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>fold_2_epoch_20_lr1e-5_batch16_seq128</td>\n",
       "      <td>128</td>\n",
       "      <td>RoBERTa-large-PM-M3-Voc/RoBERTa-large-PM-M3-Vo...</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>/home/projects/ku_10024/scratch/esmaeil/s1000/...</td>\n",
       "      <td>/home/projects/ku_10024/scratch/esmaeil/s1000/...</td>\n",
       "      <td>CMVP</td>\n",
       "      <td>0.664062</td>\n",
       "      <td>0.712291</td>\n",
       "      <td>0.687332</td>\n",
       "      <td>255</td>\n",
       "      <td>129</td>\n",
       "      <td>103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>fold_2_epoch_20_lr1e-5_batch16_seq128</td>\n",
       "      <td>128</td>\n",
       "      <td>RoBERTa-large-PM-M3-Voc/RoBERTa-large-PM-M3-Vo...</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>/home/projects/ku_10024/scratch/esmaeil/s1000/...</td>\n",
       "      <td>/home/projects/ku_10024/scratch/esmaeil/s1000/...</td>\n",
       "      <td>F</td>\n",
       "      <td>0.650131</td>\n",
       "      <td>0.695531</td>\n",
       "      <td>0.672065</td>\n",
       "      <td>249</td>\n",
       "      <td>134</td>\n",
       "      <td>109</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           experiment_ID  max_seq_length  \\\n",
       "0  fold_1_epoch_20_lr1e-5_batch16_seq128             128   \n",
       "1  fold_1_epoch_20_lr1e-5_batch16_seq128             128   \n",
       "2  fold_2_epoch_20_lr1e-5_batch16_seq128             128   \n",
       "3  fold_2_epoch_20_lr1e-5_batch16_seq128             128   \n",
       "4  fold_2_epoch_20_lr1e-5_batch16_seq128             128   \n",
       "\n",
       "                                          model_name  num_train_epochs  \\\n",
       "0  RoBERTa-large-PM-M3-Voc/RoBERTa-large-PM-M3-Vo...              20.0   \n",
       "1  RoBERTa-large-PM-M3-Voc/RoBERTa-large-PM-M3-Vo...              20.0   \n",
       "2  RoBERTa-large-PM-M3-Voc/RoBERTa-large-PM-M3-Vo...              20.0   \n",
       "3  RoBERTa-large-PM-M3-Voc/RoBERTa-large-PM-M3-Vo...              20.0   \n",
       "4  RoBERTa-large-PM-M3-Voc/RoBERTa-large-PM-M3-Vo...              20.0   \n",
       "\n",
       "   learning_rate  batch_size  predict_position  \\\n",
       "0        0.00001          16                 0   \n",
       "1        0.00001          16                 0   \n",
       "2        0.00001          16                 0   \n",
       "3        0.00001          16                 0   \n",
       "4        0.00001          16                 0   \n",
       "\n",
       "                                          train_data  \\\n",
       "0  /home/projects/ku_10024/scratch/esmaeil/s1000/...   \n",
       "1  /home/projects/ku_10024/scratch/esmaeil/s1000/...   \n",
       "2  /home/projects/ku_10024/scratch/esmaeil/s1000/...   \n",
       "3  /home/projects/ku_10024/scratch/esmaeil/s1000/...   \n",
       "4  /home/projects/ku_10024/scratch/esmaeil/s1000/...   \n",
       "\n",
       "                                           test_data method      prec  \\\n",
       "0  /home/projects/ku_10024/scratch/esmaeil/s1000/...   CMVP  0.610390   \n",
       "1  /home/projects/ku_10024/scratch/esmaeil/s1000/...      F  0.601732   \n",
       "2  /home/projects/ku_10024/scratch/esmaeil/s1000/...    CMV  0.663185   \n",
       "3  /home/projects/ku_10024/scratch/esmaeil/s1000/...   CMVP  0.664062   \n",
       "4  /home/projects/ku_10024/scratch/esmaeil/s1000/...      F  0.650131   \n",
       "\n",
       "        rec         F   TP   FP   FN  \n",
       "0  0.698020  0.651270  141   90   61  \n",
       "1  0.688119  0.642032  139   92   63  \n",
       "2  0.709497  0.685560  254  129  104  \n",
       "3  0.712291  0.687332  255  129  103  \n",
       "4  0.695531  0.672065  249  134  109  "
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/v_/y2cs3qgx37b1n03g5v0cmt5w0000gn/T/ipykernel_5221/4260414018.py:2: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  results['experiment_ID'] = results['experiment_ID'].str.replace(r'fold_[0-5]_', '')\n"
     ]
    }
   ],
   "source": [
    "# remove fold# number from experimentd ID\n",
    "results['experiment_ID'] = results['experiment_ID'].str.replace(r'fold_[0-5]_', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate total TP,FP,FN for 5 folds for different experiments and evaluation methods\n",
    "grouped_results = results.groupby(['experiment_ID', 'method'])[[ 'TP', 'FP', 'FN']].sum().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Calculate precision, recall, and F-score\n",
    "grouped_results['Precision'] = grouped_results['TP'] / (grouped_results['TP'] + grouped_results['FP'])\n",
    "grouped_results['Recall'] = grouped_results['TP'] / (grouped_results['TP'] + grouped_results['FN'])\n",
    "grouped_results['F_score'] = 2 * (grouped_results['Precision'] * grouped_results['Recall']) / (grouped_results['Precision'] + grouped_results['Recall'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_sorted = grouped_results.sort_values(by='F_score', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>experiment_ID</th>\n",
       "      <th>method</th>\n",
       "      <th>TP</th>\n",
       "      <th>FP</th>\n",
       "      <th>FN</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>epoch_60_lr1e-5_batch16_seq256</td>\n",
       "      <td>CMV</td>\n",
       "      <td>572</td>\n",
       "      <td>338</td>\n",
       "      <td>306</td>\n",
       "      <td>0.628571</td>\n",
       "      <td>0.651481</td>\n",
       "      <td>0.639821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>epoch_60_lr1e-5_batch16_seq256</td>\n",
       "      <td>CMVP</td>\n",
       "      <td>572</td>\n",
       "      <td>340</td>\n",
       "      <td>306</td>\n",
       "      <td>0.627193</td>\n",
       "      <td>0.651481</td>\n",
       "      <td>0.639106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>epoch_20_lr3e-5_batch16_seq128</td>\n",
       "      <td>CMV</td>\n",
       "      <td>696</td>\n",
       "      <td>402</td>\n",
       "      <td>396</td>\n",
       "      <td>0.633880</td>\n",
       "      <td>0.637363</td>\n",
       "      <td>0.635616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>epoch_60_lr1e-5_batch16_seq128</td>\n",
       "      <td>F</td>\n",
       "      <td>581</td>\n",
       "      <td>370</td>\n",
       "      <td>297</td>\n",
       "      <td>0.610936</td>\n",
       "      <td>0.661731</td>\n",
       "      <td>0.635320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>epoch_60_lr3e-5_batch16_seq128</td>\n",
       "      <td>CMV</td>\n",
       "      <td>553</td>\n",
       "      <td>311</td>\n",
       "      <td>325</td>\n",
       "      <td>0.640046</td>\n",
       "      <td>0.629841</td>\n",
       "      <td>0.634902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>epoch_60_lr3e-5_batch16_seq128</td>\n",
       "      <td>CMVP</td>\n",
       "      <td>558</td>\n",
       "      <td>323</td>\n",
       "      <td>320</td>\n",
       "      <td>0.633371</td>\n",
       "      <td>0.635535</td>\n",
       "      <td>0.634451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>epoch_20_lr3e-5_batch16_seq128</td>\n",
       "      <td>CMVP</td>\n",
       "      <td>699</td>\n",
       "      <td>413</td>\n",
       "      <td>393</td>\n",
       "      <td>0.628597</td>\n",
       "      <td>0.640110</td>\n",
       "      <td>0.634301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>epoch_20_lr3e-5_batch16_seq128</td>\n",
       "      <td>F</td>\n",
       "      <td>702</td>\n",
       "      <td>428</td>\n",
       "      <td>390</td>\n",
       "      <td>0.621239</td>\n",
       "      <td>0.642857</td>\n",
       "      <td>0.631863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>epoch_50_lr3e-5_batch16_seq256</td>\n",
       "      <td>CMVP</td>\n",
       "      <td>687</td>\n",
       "      <td>408</td>\n",
       "      <td>393</td>\n",
       "      <td>0.627397</td>\n",
       "      <td>0.636111</td>\n",
       "      <td>0.631724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>epoch_40_lr1e-5_batch16_seq128</td>\n",
       "      <td>CMVP</td>\n",
       "      <td>1289</td>\n",
       "      <td>826</td>\n",
       "      <td>681</td>\n",
       "      <td>0.609456</td>\n",
       "      <td>0.654315</td>\n",
       "      <td>0.631089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>epoch_50_lr3e-5_batch16_seq256</td>\n",
       "      <td>CMV</td>\n",
       "      <td>681</td>\n",
       "      <td>403</td>\n",
       "      <td>399</td>\n",
       "      <td>0.628229</td>\n",
       "      <td>0.630556</td>\n",
       "      <td>0.629390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>epoch_40_lr1e-5_batch16_seq128</td>\n",
       "      <td>CMV</td>\n",
       "      <td>1276</td>\n",
       "      <td>815</td>\n",
       "      <td>694</td>\n",
       "      <td>0.610234</td>\n",
       "      <td>0.647716</td>\n",
       "      <td>0.628417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>epoch_60_lr3e-5_batch16_seq128</td>\n",
       "      <td>F</td>\n",
       "      <td>559</td>\n",
       "      <td>343</td>\n",
       "      <td>319</td>\n",
       "      <td>0.619734</td>\n",
       "      <td>0.636674</td>\n",
       "      <td>0.628090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>epoch_40_lr1e-5_batch16_seq128</td>\n",
       "      <td>F</td>\n",
       "      <td>1290</td>\n",
       "      <td>853</td>\n",
       "      <td>680</td>\n",
       "      <td>0.601960</td>\n",
       "      <td>0.654822</td>\n",
       "      <td>0.627279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>epoch_60_lr1e-5_batch16_seq256</td>\n",
       "      <td>F</td>\n",
       "      <td>565</td>\n",
       "      <td>362</td>\n",
       "      <td>313</td>\n",
       "      <td>0.609493</td>\n",
       "      <td>0.643508</td>\n",
       "      <td>0.626039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>epoch_20_lr1e-5_batch16_seq128</td>\n",
       "      <td>CMVP</td>\n",
       "      <td>716</td>\n",
       "      <td>483</td>\n",
       "      <td>376</td>\n",
       "      <td>0.597164</td>\n",
       "      <td>0.655678</td>\n",
       "      <td>0.625055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>epoch_40_lr3e-5_batch16_seq128</td>\n",
       "      <td>CMVP</td>\n",
       "      <td>1229</td>\n",
       "      <td>737</td>\n",
       "      <td>741</td>\n",
       "      <td>0.625127</td>\n",
       "      <td>0.623858</td>\n",
       "      <td>0.624492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>epoch_60_lr3e-5_batch16_seq256</td>\n",
       "      <td>CMVP</td>\n",
       "      <td>540</td>\n",
       "      <td>313</td>\n",
       "      <td>338</td>\n",
       "      <td>0.633060</td>\n",
       "      <td>0.615034</td>\n",
       "      <td>0.623917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>epoch_60_lr1e-5_batch16_seq128</td>\n",
       "      <td>CMV</td>\n",
       "      <td>569</td>\n",
       "      <td>380</td>\n",
       "      <td>309</td>\n",
       "      <td>0.599579</td>\n",
       "      <td>0.648064</td>\n",
       "      <td>0.622879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>epoch_60_lr3e-5_batch16_seq256</td>\n",
       "      <td>CMV</td>\n",
       "      <td>537</td>\n",
       "      <td>310</td>\n",
       "      <td>341</td>\n",
       "      <td>0.634002</td>\n",
       "      <td>0.611617</td>\n",
       "      <td>0.622609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>epoch_50_lr1e-5_batch16_seq128</td>\n",
       "      <td>F</td>\n",
       "      <td>1276</td>\n",
       "      <td>854</td>\n",
       "      <td>694</td>\n",
       "      <td>0.599061</td>\n",
       "      <td>0.647716</td>\n",
       "      <td>0.622439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>epoch_40_lr3e-5_batch16_seq128</td>\n",
       "      <td>CMV</td>\n",
       "      <td>1222</td>\n",
       "      <td>735</td>\n",
       "      <td>748</td>\n",
       "      <td>0.624425</td>\n",
       "      <td>0.620305</td>\n",
       "      <td>0.622358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>epoch_30_lr3e-5_batch16_seq256</td>\n",
       "      <td>F</td>\n",
       "      <td>686</td>\n",
       "      <td>427</td>\n",
       "      <td>406</td>\n",
       "      <td>0.616352</td>\n",
       "      <td>0.628205</td>\n",
       "      <td>0.622222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>epoch_60_lr1e-5_batch16_seq128</td>\n",
       "      <td>CMVP</td>\n",
       "      <td>571</td>\n",
       "      <td>389</td>\n",
       "      <td>307</td>\n",
       "      <td>0.594792</td>\n",
       "      <td>0.650342</td>\n",
       "      <td>0.621328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>epoch_50_lr1e-5_batch16_seq128</td>\n",
       "      <td>CMV</td>\n",
       "      <td>1263</td>\n",
       "      <td>833</td>\n",
       "      <td>707</td>\n",
       "      <td>0.602576</td>\n",
       "      <td>0.641117</td>\n",
       "      <td>0.621249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>epoch_50_lr1e-5_batch16_seq256</td>\n",
       "      <td>CMV</td>\n",
       "      <td>1264</td>\n",
       "      <td>836</td>\n",
       "      <td>706</td>\n",
       "      <td>0.601905</td>\n",
       "      <td>0.641624</td>\n",
       "      <td>0.621130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>epoch_50_lr1e-5_batch16_seq128</td>\n",
       "      <td>CMVP</td>\n",
       "      <td>1271</td>\n",
       "      <td>855</td>\n",
       "      <td>699</td>\n",
       "      <td>0.597836</td>\n",
       "      <td>0.645178</td>\n",
       "      <td>0.620605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>epoch_50_lr1e-5_batch16_seq256</td>\n",
       "      <td>CMVP</td>\n",
       "      <td>1266</td>\n",
       "      <td>846</td>\n",
       "      <td>704</td>\n",
       "      <td>0.599432</td>\n",
       "      <td>0.642640</td>\n",
       "      <td>0.620284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>epoch_40_lr1e-5_batch16_seq256</td>\n",
       "      <td>F</td>\n",
       "      <td>1279</td>\n",
       "      <td>880</td>\n",
       "      <td>691</td>\n",
       "      <td>0.592404</td>\n",
       "      <td>0.649239</td>\n",
       "      <td>0.619520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>epoch_40_lr3e-5_batch16_seq256</td>\n",
       "      <td>CMV</td>\n",
       "      <td>1222</td>\n",
       "      <td>762</td>\n",
       "      <td>748</td>\n",
       "      <td>0.615927</td>\n",
       "      <td>0.620305</td>\n",
       "      <td>0.618108</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     experiment_ID method    TP   FP   FN  Precision  \\\n",
       "51  epoch_60_lr1e-5_batch16_seq256    CMV   572  338  306   0.628571   \n",
       "52  epoch_60_lr1e-5_batch16_seq256   CMVP   572  340  306   0.627193   \n",
       "6   epoch_20_lr3e-5_batch16_seq128    CMV   696  402  396   0.633880   \n",
       "50  epoch_60_lr1e-5_batch16_seq128      F   581  370  297   0.610936   \n",
       "54  epoch_60_lr3e-5_batch16_seq128    CMV   553  311  325   0.640046   \n",
       "55  epoch_60_lr3e-5_batch16_seq128   CMVP   558  323  320   0.633371   \n",
       "7   epoch_20_lr3e-5_batch16_seq128   CMVP   699  413  393   0.628597   \n",
       "8   epoch_20_lr3e-5_batch16_seq128      F   702  428  390   0.621239   \n",
       "46  epoch_50_lr3e-5_batch16_seq256   CMVP   687  408  393   0.627397   \n",
       "25  epoch_40_lr1e-5_batch16_seq128   CMVP  1289  826  681   0.609456   \n",
       "45  epoch_50_lr3e-5_batch16_seq256    CMV   681  403  399   0.628229   \n",
       "24  epoch_40_lr1e-5_batch16_seq128    CMV  1276  815  694   0.610234   \n",
       "56  epoch_60_lr3e-5_batch16_seq128      F   559  343  319   0.619734   \n",
       "26  epoch_40_lr1e-5_batch16_seq128      F  1290  853  680   0.601960   \n",
       "53  epoch_60_lr1e-5_batch16_seq256      F   565  362  313   0.609493   \n",
       "1   epoch_20_lr1e-5_batch16_seq128   CMVP   716  483  376   0.597164   \n",
       "31  epoch_40_lr3e-5_batch16_seq128   CMVP  1229  737  741   0.625127   \n",
       "58  epoch_60_lr3e-5_batch16_seq256   CMVP   540  313  338   0.633060   \n",
       "48  epoch_60_lr1e-5_batch16_seq128    CMV   569  380  309   0.599579   \n",
       "57  epoch_60_lr3e-5_batch16_seq256    CMV   537  310  341   0.634002   \n",
       "38  epoch_50_lr1e-5_batch16_seq128      F  1276  854  694   0.599061   \n",
       "30  epoch_40_lr3e-5_batch16_seq128    CMV  1222  735  748   0.624425   \n",
       "23  epoch_30_lr3e-5_batch16_seq256      F   686  427  406   0.616352   \n",
       "49  epoch_60_lr1e-5_batch16_seq128   CMVP   571  389  307   0.594792   \n",
       "36  epoch_50_lr1e-5_batch16_seq128    CMV  1263  833  707   0.602576   \n",
       "39  epoch_50_lr1e-5_batch16_seq256    CMV  1264  836  706   0.601905   \n",
       "37  epoch_50_lr1e-5_batch16_seq128   CMVP  1271  855  699   0.597836   \n",
       "40  epoch_50_lr1e-5_batch16_seq256   CMVP  1266  846  704   0.599432   \n",
       "29  epoch_40_lr1e-5_batch16_seq256      F  1279  880  691   0.592404   \n",
       "33  epoch_40_lr3e-5_batch16_seq256    CMV  1222  762  748   0.615927   \n",
       "\n",
       "      Recall   F_score  \n",
       "51  0.651481  0.639821  \n",
       "52  0.651481  0.639106  \n",
       "6   0.637363  0.635616  \n",
       "50  0.661731  0.635320  \n",
       "54  0.629841  0.634902  \n",
       "55  0.635535  0.634451  \n",
       "7   0.640110  0.634301  \n",
       "8   0.642857  0.631863  \n",
       "46  0.636111  0.631724  \n",
       "25  0.654315  0.631089  \n",
       "45  0.630556  0.629390  \n",
       "24  0.647716  0.628417  \n",
       "56  0.636674  0.628090  \n",
       "26  0.654822  0.627279  \n",
       "53  0.643508  0.626039  \n",
       "1   0.655678  0.625055  \n",
       "31  0.623858  0.624492  \n",
       "58  0.615034  0.623917  \n",
       "48  0.648064  0.622879  \n",
       "57  0.611617  0.622609  \n",
       "38  0.647716  0.622439  \n",
       "30  0.620305  0.622358  \n",
       "23  0.628205  0.622222  \n",
       "49  0.650342  0.621328  \n",
       "36  0.641117  0.621249  \n",
       "39  0.641624  0.621130  \n",
       "37  0.645178  0.620605  \n",
       "40  0.642640  0.620284  \n",
       "29  0.649239  0.619520  \n",
       "33  0.620305  0.618108  "
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_sorted.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_sorted.to_csv('../../data/s1000_ner/results/results-new-grid-summed.csv',sep='\\t',index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Best hyperparameter set:\n",
    "    * epoch_60_lr1e-5_batch16_seq256\t\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final train "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Note: I combined this part with Transformer Tagger in the next section, which trains at the same time and predicts for test data using tagger format input\n",
    "* So we can skipp this standalone training\n",
    "* Train model using best hyperparameter set found in the previous section\n",
    "    *     epoch_60_lr1e-5_batch16_seq256\t\n",
    "* Train is done by the the whole 80% of dataset which was initially splitted to folds for grid search and now combined\n",
    "    * /home/projects/ku_10024/scratch/esmaeil/s1000/S1000-transformer-ner/data/balanced/train_data_merged_with_LSF_branches.tsv\n",
    "* Test is 20% hold out\n",
    "    * /home/projects/ku_10024/scratch/esmaeil/s1000/S1000-transformer-ner/data/balanced/test_data_merged_with_LSF_branches.tsv\n",
    "* 1. ssh C2\n",
    "* 2. cd /home/projects/ku_10024/scratch/esmaeil/s1000/S1000-transformer-ner/scripts\n",
    "* 3. qsub qsub_ner_train_test_with_LSF_branches.sh\n",
    "* 4. trained model is located in /home/projects/ku_10024/scratch/esmaeil/s1000/S1000-transformer-ner/S1000-transformer-ner-with-LSF-branches that will be used in next step by transformer tagger to produce final NER prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# S1000 Transformer Tagger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Note: Results of Transformer NER can be evaulated in 2 different ways:\n",
    "    * 1. Evaluation script which is available in the repo of NER and by default is calculated (the approach used to do grid search)\n",
    "    * 2. There is Transformet Taggger that uses the fine tuned model in the first step to tag or detect entities in the text\n",
    "            * This approach recieves the input in a string db format (6 columns tsv file where each row is an abstract and first column is pmid abd the last column is the asbtract text)\n",
    "            * This creates predictions in tagger format which is a tsv file for mentions and their offset in abstract and etc. \n",
    "            * This file can be converted to corresponding *.ann files similar to what we did for tagger macthes and then we can compare manual annotations with predicted *.ann similar to what we did for benchmarking\n",
    "            * This approach produces improved results because we use -o argument which considers overlaping matches as match, without this arguument the result will be decreased significantly(similar can happen to tagger results) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "* clone repo:\n",
    "    * cd /home/projects/ku_10024/scratch/esmaeil/s1000\n",
    "    * git clone https://github.com/jouniluoma/S1000-transformer-tagger.git\n",
    "* download sample data:\n",
    "    * wget https://a3s.fi/s1000/database_sample.tsv.gz\n",
    "    * mkdir data\n",
    "    * mv database_sample.tsv.gz data/\n",
    "    * gunzip data/database_sample.tsv.gz\n",
    "* modify scripts/run-bio-tagger.sh to point to our finetuned model in previous step\n",
    "    * ner_model=\"../S1000-transformer-ner/S1000-transformer-ner/\"\n",
    "    * I did this to sole the spacy issue:\n",
    "        * pip uninstall spacy\n",
    "        * pip uninstall pydantic\n",
    "        * pip install spacy pydantic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create single tsv file using test txt files\n",
    "    * there will be 6 tab separated columns, first is the abstract id and last is the contnt, other are just be compatible with requested format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def create_single_tsv_from_txt_files(input_dir, output_tsv_file):\n",
    "    # Filler dummy words for the middle columns\n",
    "    dummy_words = [\"other_ids\", \"authors\", \"forum\", \"year\"]\n",
    "\n",
    "    # Initialize an empty list to store the TSV lines\n",
    "    tsv_lines = []\n",
    "\n",
    "    # Loop through the TXT files in the input directory\n",
    "    for filename in os.listdir(input_dir):\n",
    "        if filename.endswith(\".txt\"):\n",
    "            # Read the content of the TXT file without modifications\n",
    "            with open(os.path.join(input_dir, filename), 'r') as txt_file:\n",
    "                file_content = txt_file.read().strip()\n",
    "\n",
    "                # Remove newline characters from the file content\n",
    "                #file_content = file_content.replace('\\n', ' ').replace('\\r', '')\n",
    "                file_content = file_content.replace('\\n', '')\n",
    "\n",
    "            # Create a TSV line for the current file\n",
    "            tsv_line = [filename] + dummy_words + ['\"' + file_content + '\"']\n",
    "            tsv_lines.append(\"\\t\".join(tsv_line))\n",
    "\n",
    "    # Write the TSV lines to the output file\n",
    "    with open(output_tsv_file, 'w') as output_file:\n",
    "        output_file.write(\"\\n\".join(tsv_lines))\n",
    "\n",
    "    print(f\"TSV file '{output_tsv_file}' created successfully.\")\n",
    "\n",
    "# # Example usage:\n",
    "# # Replace input_dir and output_tsv_file with your desired directory and file paths\n",
    "# input_dir = '../../data/s1000_ner/test_data/'\n",
    "# output_tsv_file = '../../data/s1000_ner/test_data_merged_Tagger_format.tsv'\n",
    "# create_single_tsv_from_txt_files(input_dir, output_tsv_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TSV file '../../data/s1000_ner/balanced_LSF_types_splits/test_data_merged_Tagger_format.tsv' created successfully.\n"
     ]
    }
   ],
   "source": [
    "input_dir = '../../data/s1000_ner/balanced_LSF_types_splits/test_data/'\n",
    "output_tsv_file = '../../data/s1000_ner/balanced_LSF_types_splits/test_data_merged_Tagger_format.tsv'\n",
    "create_single_tsv_from_txt_files(input_dir, output_tsv_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get predictions using S1000 Transformer Tagger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Transfer the file to  computerome\n",
    "    * /home/projects/ku_10024/scratch/esmaeil/s1000/S1000-transformer-tagger/data/test_data_merged_Tagger_format.tsv\n",
    "    * \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Run qsub:\n",
    "    * cd /home/projects/ku_10024/scratch/esmaeil/s1000/S1000-transformer-tagger/scripts\n",
    "    * qsub  qsub-final.sh\n",
    "    * qsub-final.sh contains both training the model and applying the trained model for getting predictions for test data in tagger format\n",
    "    * out put will be in /home/projects/ku_10024/scratch/esmaeil/s1000/S1000-transformer-tagger/output/output-with-LSF-branches-balanced-spans.tsv\n",
    "        * this is the tagged entities in tagger format we can convert it ti correspoinding *.ann files to make it easier to compare with manual annotations\n",
    "    * trained model will be /S1000-transformer-ner/S1000-transformer-ner-LSF-branches-balanced-final/\n",
    "* Alternative using interactive session (Obselete)\n",
    "    * run script\n",
    "        * create iqsub session \n",
    "        * module load tools\n",
    "        * cd /home/projects/ku_10024/scratch/esmaeil/s1000\n",
    "        * conda  activate /home/projects/ku_10024/scratch/esmaeil/s1000/s1000\n",
    "        * cd S1000-transformer-tagger/\n",
    "        * ./scripts/run-bio-tagger.sh\n",
    "        * this will create ouputs in \n",
    "            * /home/projects/ku_10024/scratch/esmaeil/s1000/S1000-transformer-tagger/output/output-LSF-spans.tsv\n",
    "            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert predictions to *.ann files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def create_ann_files_from_NER_output(input_file, output_ann_folder):\n",
    "    # Read the input file, remove duplicates, and write to the output file\n",
    "    df = pd.read_csv(input_file, sep='\\t', header=None)\n",
    "    df.drop_duplicates(inplace=True)\n",
    "    df.to_csv(input_file, index=False, header=None, sep='\\t')\n",
    "\n",
    "    os.makedirs(output_ann_folder, exist_ok=True)\n",
    "\n",
    "    previous_value = None\n",
    "    counter = 1\n",
    "\n",
    "    with open(input_file, 'r', encoding='utf-8-sig') as input_file:\n",
    "        for line in input_file:\n",
    "            fields = line.strip().split('\\t')\n",
    "            current_value = fields[0]\n",
    "            # Remove .txt extension from filename\n",
    "            current_value = current_value.rstrip()[:-4]\n",
    "\n",
    "            if current_value != previous_value:\n",
    "                counter = 1  # Reset the counter for a new value in the first column\n",
    "                previous_value = current_value\n",
    "\n",
    "            filename = os.path.join(output_ann_folder, current_value + '.ann')\n",
    "            with open(filename, 'a', encoding='utf-8') as output_file:\n",
    "                span_text = f'{fields[6]} {int(fields[3])-1} {int(fields[4])}'\n",
    "                span_text = span_text.replace('\\xa0', ' ')  # Replace non-breaking space with a normal space\n",
    "                output_file.write(f'T{counter}\\t{span_text}\\t{fields[5]}\\n')\n",
    "\n",
    "            counter += 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine tuning the model using single LSF type ignoring LSF types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Example usage:\n",
    "# input_file = '../../data/s1000_ner/output/output-ignore-types-balanced-spans.tsv'\n",
    "# output_ann_folder = '../../data/s1000_ner/output/transformer_predicted_ann_files_single_LSF_type/'\n",
    "# create_ann_files_from_NER_output(input_file, output_ann_folder)\n",
    "#!python2 ../../../utils/IAA.py -o -d -v  -i  '../../data/s1000_ner/balanced_LSF_types_splits/test_data'   '../../data/s1000_ner/output/transformer_predicted_ann_files_single_LSF_type/' --allowmissing > ../../data/s1000_ner/results/performance_single_LSF_type.txt 2>&1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine tuning the model using different LSF types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "#input_file='../../data/s1000_ner/output/output-with-LSF-branches-spans.tsv'\n",
    "input_file='../../data/s1000_ner/output/output-with-LSF-branches-balanced-spans.tsv'\n",
    "\n",
    "df=pd.read_csv(input_file,sep='\\t',header=None)\n",
    "df.drop_duplicates(inplace=True)\n",
    "df.to_csv(input_file,index=False,header=None,sep='\\t')\n",
    "\n",
    "\n",
    "#output_ann_folder = '../../data/s1000_ner/output/transformer_predicted_ann_files_with_LSF_types/'\n",
    "output_ann_folder = '../../data/s1000_ner/output/transformer_predicted_ann_files_with_LSF_types_balanced/'\n",
    "os.makedirs(output_ann_folder, exist_ok=True)\n",
    "\n",
    "\n",
    "previous_value = None\n",
    "counter = 1\n",
    "\n",
    "with open(input_file, 'r', encoding='utf-8-sig') as input_file:  # Use utf-8-sig to handle BOM\n",
    "    for line in input_file:\n",
    "        fields = line.strip().split('\\t')\n",
    "        current_value = fields[0]\n",
    "        # remove .txt extension from filename\n",
    "        current_value = current_value.rstrip()[:-4] \n",
    "        \n",
    "        if current_value != previous_value:\n",
    "            counter = 1  # Reset the counter for a new value in the first column\n",
    "            previous_value = current_value\n",
    "        \n",
    "        filename = os.path.join(output_ann_folder, current_value + '.ann')\n",
    "        with open(filename, 'a', encoding='utf-8') as output_file:  # Ensure utf-8 encoding without BOM\n",
    "                #span_text = f'{fields[6]} {int(fields[3])-1} {int(fields[4])}'\n",
    "                span_text = f'{fields[6]} {int(fields[3])-1} {int(fields[4])}'\n",
    "                span_text = span_text.replace('\\xa0', ' ')  # Replace non-breaking space with normal space\n",
    "                output_file.write(f'T{counter}\\t{span_text}\\t{fields[5]}\\n')\n",
    "        \n",
    "        counter += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create empty ann files for those abstracts that don't have them because NER has not found any matches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import os\n",
    "\n",
    "def create_missing_ann_files(ann_dir,txt_dir):\n",
    "    for filename in os.listdir(txt_dir):\n",
    "        if filename.endswith('.txt'):\n",
    "            txt_file_path = os.path.join(ann_dir, filename)\n",
    "            ann_file_path = os.path.splitext(txt_file_path)[0] + '.ann'\n",
    "            \n",
    "            if not os.path.exists(ann_file_path):\n",
    "                with open(ann_file_path, 'w') as ann_file:\n",
    "                    pass  # Creates an empty .ann file\n",
    "\n",
    "ann_dir = '../../data/s1000_ner/output/transformer_predicted_ann_files_with_LSF_types_balanced/'\n",
    "txt_dir ='../../data/s1000_ner/balanced_LSF_types_splits/test_data/'\n",
    "\n",
    "create_missing_ann_files(ann_dir,txt_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### calculate performance in comparison with manual annotations:\n",
    "    * This is based on filtered annotation files\n",
    "    * keywords_removed = {\"LSF_out_of_context\", \"Geographical_Feature\", \"Occupations\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "!python2 ../../../utils/IAA.py -o -d -v  '../../data/s1000_ner/balanced_LSF_types_splits/test_data/'   '../../data/s1000_ner/output/transformer_predicted_ann_files_with_LSF_types_balanced/' --allowmissing > ../../data/s1000_ner/results/performance_with_LSF_types_balanced.txt 2>&1\n",
    "!grep 'FP:' ../../data/s1000_ner/results/performance_with_LSF_types_balanced.txt > ../../data/s1000_ner/results/performance_with_LSF_types_balanced_FP_lines.txt\n",
    "!grep 'FN:' ../../data/s1000_ner/results/performance_with_LSF_types_balanced.txt > ../../data/s1000_ner/results/performance_with_LSF_types_balanced_FN_lines.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python2 ../../../utils/IAA.py -o -d -v  -i  '../../data/s1000_ner/balanced_LSF_types_splits/test_data/'   '../../data/s1000_ner/output/transformer_predicted_ann_files_with_LSF_types_balanced/' --allowmissing > ../../data/s1000_ner/results/performance_with_LSF_types_ignored_balanced.txt 2>&1\n",
    "!grep 'FP:' ../../data/s1000_ner/results/performance_with_LSF_types_ignored_balanced.txt > ../../data/s1000_ner/results/performance_with_LSF_types_ignored_balanced_FP_lines.txt\n",
    "!grep 'FN:' ../../data/s1000_ner/results/performance_with_LSF_types_ignored_balanced.txt > ../../data/s1000_ner/results/performance_with_LSF_types_ignored_balanced_FN_lines.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modified file '../../data/s1000_ner/results/performance_with_LSF_types_ignored_balanced_FP_lines_with_links.txt' created.\n",
      "Modified file '../../data/s1000_ner/results/performance_with_LSF_types_ignored_balanced_FN_lines_with_links.txt' created.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def add_links(input_file, output_file,correction):\n",
    "    with open(input_file, 'r') as infile, open(output_file, 'w') as outfile:\n",
    "        for line in infile:\n",
    "            parts = line.strip().split('\\t')\n",
    "            if len(parts) >= 1:\n",
    "                first_column = parts[0]\n",
    "                elements = first_column.split()\n",
    "                if len(elements) >= 3:\n",
    "                    identifier = elements[0]\n",
    "                    code = elements[-1]\n",
    "\n",
    "                    # Extract the offsets from the second tab-separated column\n",
    "                    offset_parts = parts[1].split()[1:]\n",
    "                    if len(offset_parts) >= 2:\n",
    "                        start_offset, end_offset = map(int, offset_parts)\n",
    "                        start_offset += correction\n",
    "                        end_offset += correction\n",
    "                        second_column = f\"http://localhost:5555/index.xhtml#/NER/Annotated_200_abstracts/{identifier}?focus={start_offset}~{end_offset}\"\n",
    "                        outfile.write(first_column + '\\t' + second_column + '\\t' + '\\t'.join(parts[2:]) + '\\n')\n",
    "                    else:\n",
    "                        outfile.write(line)\n",
    "                else:\n",
    "                    outfile.write(line)\n",
    "            else:\n",
    "                outfile.write(line)\n",
    "\n",
    "    print(f\"Modified file '{output_file}' created.\")\n",
    "\n",
    "# Example usage for FP\n",
    "input_file_fp = r\"../../data/s1000_ner/results/performance_with_LSF_types_ignored_balanced_FP_lines.txt\"\n",
    "output_file_fp = r\"../../data/s1000_ner/results/performance_with_LSF_types_ignored_balanced_FP_lines_with_links.txt\"\n",
    "add_links(input_file_fp, output_file_fp,correction=1)\n",
    "\n",
    "# Example usage for FN\n",
    "input_file_fn = r\"../../data/s1000_ner/results/performance_with_LSF_types_ignored_balanced_FN_lines.txt\"\n",
    "output_file_fn = r\"../../data/s1000_ner/results/performance_with_LSF_types_ignored_balanced_FN_lines_with_links.txt\"\n",
    "add_links(input_file_fn, output_file_fn,correction=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### produce tagger matches using dictionary based-NER-matches for test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python2 ../../../utils/IAA.py -o -d -v   '../../data/s1000_ner/balanced_LSF_types_splits/test_data/'   \"../../data/200_abstracts/taggers_matches/brat_files/\" --allowmissing > ../../data/s1000_ner/results/performance_Tagger_for_Test_Data.txt 2>&1\n",
    "!python2 ../../../utils/IAA.py -o -d -v   '../../data/s1000_ner/balanced_LSF_types_splits/test_data/'   \"../../data/200_abstracts/taggers_matches/brat_files/\"  > ../../data/s1000_ner/results/performance_Tagger_for_Test_Data.txt 2>&1\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* extract metrics from text files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def extract_data_and_save(file_path):\n",
    "    # Initialize an empty DataFrame\n",
    "    pr_rec_per_lsf_branch = pd.DataFrame(columns=['Lifestyle-factor branch', 'Precision', 'Recall', 'F'])\n",
    "\n",
    "    # Read the text from the file\n",
    "    with open(file_path, 'r') as file:\n",
    "        text = file.read()\n",
    "\n",
    "    # Split the text into lines\n",
    "    lines = text.strip().split('\\n')\n",
    "\n",
    "    # Iterate over each line and extract data\n",
    "    for line in lines:\n",
    "        match = re.match(r'TYPE:\\s+(.*?)\\s+precision\\s+([\\d.]+%)\\s+\\((\\d+)/(\\d+)\\)\\s+recall\\s+([\\d.]+%)\\s+\\((\\d+)/(\\d+)\\)\\s+F\\s+([\\d.]+)', line)\n",
    "        if match:\n",
    "            revision_step, precision, _, _, recall, _, _, f_score = match.groups()\n",
    "            if revision_step not in ['Beauty_and_Cleaning', 'Drugs', 'Environmental_exposures',\n",
    "                                     'Mental_health_practices', 'Non_physical_leisure_time_activities',\n",
    "                                     'Nutrition', 'Physical_activity', 'Sleep', 'Socioeconomic_factors']:\n",
    "                continue\n",
    "\n",
    "            # Remove the percentage symbols and convert to floats\n",
    "            precision = float(precision.rstrip('%'))\n",
    "            recall = float(recall.rstrip('%'))\n",
    "            f_score = float(f_score)\n",
    "            # Append the data to the DataFrame\n",
    "            pr_rec_per_lsf_branch = pr_rec_per_lsf_branch.append({\n",
    "                'Lifestyle-factor branch': revision_step.strip(),\n",
    "                'Precision': precision,\n",
    "                'Recall': recall,\n",
    "                'F': f_score\n",
    "            }, ignore_index=True)\n",
    "\n",
    "    # Sort the DataFrame by the 'F' column in ascending order\n",
    "    pr_rec_per_lsf_branch = pr_rec_per_lsf_branch.sort_values(by='F')\n",
    "    \n",
    "    # Return the resulting DataFrame\n",
    "    return pr_rec_per_lsf_branch\n",
    "\n",
    "# full_output Dictionary based NER for test data:\n",
    "file_path = \"../../data/s1000_ner/results/performance_Tagger_for_Test_Data.txt\"\n",
    "\n",
    "resulting_df = extract_data_and_save(file_path)\n",
    "# Save the DataFrame to a TSV file\n",
    "tsv_file_path = \"../../data/s1000_ner/results/performance_Tagger_for_Test_Data_prec_rec.tsv\"\n",
    "resulting_df.to_csv(tsv_file_path, sep='\\t', index=None)\n",
    "\n",
    "\n",
    "\n",
    "# full_output S1000 NER for test data:\n",
    "file_path = \"../../data/s1000_ner/results/performance_with_LSF_types_balanced.txt\"\n",
    "resulting_df = extract_data_and_save(file_path)\n",
    "# Save the DataFrame to a TSV file\n",
    "tsv_file_path = \"../../data/s1000_ner/results/performance_Deep_for_Test_Data_prec_rec.tsv\"\n",
    "resulting_df.to_csv(tsv_file_path, sep='\\t', index=None)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Modify lables to be appropriate for plotting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "\n",
    "def replace_phrases_in_dataframe(input_file_path, phrase_replacements):\n",
    "    # Read the input TSV file into a DataFrame\n",
    "    df = pd.read_csv(input_file_path, sep='\\t')\n",
    "\n",
    "    # Replace the phrases in the DataFrame\n",
    "    df['Lifestyle-factor branch'] = df['Lifestyle-factor branch'].replace(phrase_replacements)\n",
    "\n",
    "    # Write the modified DataFrame to the output TSV file\n",
    "    df.to_csv(input_file_path, sep='\\t', index=False)\n",
    "\n",
    "\n",
    "phrase_replacements = {\n",
    "    'Environmental_exposures': 'Environmental exposures',\n",
    "    'Physical_activity': 'Physical activities',\n",
    "    'Socioeconomic_factors': 'Socioeconomic factors',\n",
    "    'Drugs': 'Substance use',\n",
    "    'Mental_health_practices': 'Mental health practices',\n",
    "    'Non_physical_leisure_time_activities': 'Non physical leisure time activities',\n",
    "    'Beauty_and_Cleaning': 'Beauty and cleaning'\n",
    "}\n",
    "\n",
    "\n",
    "# replace names in both files\n",
    "Tagger_file = \"../../data/s1000_ner/results/performance_Tagger_for_Test_Data_prec_rec.tsv\"\n",
    "replace_phrases_in_dataframe(Tagger_file, phrase_replacements)\n",
    "\n",
    "Transformer_NER_file = \"../../data/s1000_ner/results/performance_Deep_for_Test_Data_prec_rec.tsv\"\n",
    "replace_phrases_in_dataframe(Transformer_NER_file, phrase_replacements)\n",
    "\n",
    "\n",
    "# merge two files\n",
    "\n",
    "df_tagger = pd.read_csv(Tagger_file, sep='\\t')\n",
    "df2_transformer = pd.read_csv(Transformer_NER_file, sep='\\t')\n",
    "df2_transformer.columns = ['Lifestyle-factor branch']+[col + '_transformer' for col in df2_transformer.columns if col!='Lifestyle-factor branch']\n",
    "\n",
    "merged_df = df_tagger.merge(df2_transformer, on='Lifestyle-factor branch')\n",
    "\n",
    "# Save the merged DataFrame to a new TSV file\n",
    "merged_file = \"../../data/s1000_ner/results/plot_input_compare_DIC_VS_Transformer.tsv\"\n",
    "\n",
    "#merged_df=merged_df.sort_values(by=['Lifestyle-factor branch'])\n",
    "\n",
    "desired_order = ['nutrition', 'socioeconomic factors', 'environmental exposures', 'substance use','physical activities', 'beauty and cleaning',  'non physical leisure time activities',  'sleep' , 'mental health practices']\n",
    "# Create a new column with the order of 'Lifestyle-factor branch' (ignoring case)\n",
    "merged_df['Order'] = merged_df['Lifestyle-factor branch'].str.lower().map({value.lower(): index for index, value in enumerate(desired_order)})\n",
    "# Sort the DataFrame based on the new 'Order' column\n",
    "merged_df_sorted = merged_df.sort_values(by='Order').drop('Order', axis=1)\n",
    "merged_df_sorted.to_csv(merged_file, sep='\\t', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    * plot by comparing Tagger Ner with Deep NER\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Figure(1200x1200)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#!python3 ../plot_prec_rec_Deep_vs_Tagger.py --input_file=../../data/s1000_ner/results/plot_input_compare_DIC_VS_Transformer.tsv --task=\"Performance of Transformer-based and Dictonary-based NER (Test-Data)\" --output_file=../../data/s1000_ner/results/plot_Tagger_VS_Transformer_NER.png\n",
    "!python3 ../plot_prec_rec_Deep_vs_Tagger.py --input_file=../../data/s1000_ner/results/plot_input_compare_DIC_VS_Transformer.tsv --task=\"\" --output_file=../../data/s1000_ner/results/plot_Tagger_VS_Transformer_NER.png\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lsf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
