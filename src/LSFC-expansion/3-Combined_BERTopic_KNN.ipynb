{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import sys, os\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('../..'))\n",
    "if module_path not in sys.path:\n",
    "     sys.path.append(module_path+\"//utils\")\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import semantic_term_placement\n",
    "\n",
    "from collections import Counter\n",
    "from bertopic import BERTopic\n",
    "from sklearn.isotonic import IsotonicRegression\n",
    "\n",
    "from collections import deque\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import PrecisionRecallDisplay\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Test Candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({-1: 110, 1: 15, 3: 9, 0: 3, 2: 3, 5: 2, 8: 2, 6: 1})\n"
     ]
    }
   ],
   "source": [
    "def load_test_data(file_path):\n",
    "    test_candidates=pd.read_csv(file_path,sep='\\t')\n",
    "    test_labels=test_candidates.Label.to_numpy()\n",
    "    test_candidates_context=test_candidates.top_terms.to_numpy()\n",
    "    test_candidates_names=test_candidates.name.to_numpy()\n",
    "\n",
    "    label_distribution=Counter(test_labels)\n",
    "    print(label_distribution)\n",
    "    return test_candidates_names,test_candidates_context,test_labels\n",
    "\n",
    "\n",
    "def generate_binary_label(test_candidates):\n",
    "    lsf_labels=list(range (9))\n",
    "    # assign 1 to LSF samples and 0 to non-LSF samples\n",
    "    binary_labels=[]\n",
    "    lsf_indices=[]\n",
    "    non_lsf_indices=[]\n",
    "    for i,label in enumerate(test_candidates):\n",
    "        if label in lsf_labels:\n",
    "            lsf_indices.append(i)\n",
    "            binary_labels.append(1)\n",
    "        else:\n",
    "            non_lsf_indices.append(i)\n",
    "            binary_labels.append(0)\n",
    "    ratio= math.ceil (len(non_lsf_indices)/len(lsf_indices))\n",
    "    return binary_labels,ratio\n",
    "\n",
    "\n",
    "\n",
    "file_path='../../data/test/test.tsv'\n",
    "test_candidates_names,test_candidates_context,test_labels=load_test_data(file_path)\n",
    "binary_labels,ratio=generate_binary_label(test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict using Annoy Index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creaet Annoy Index "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Create two indices\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    * Context around each sample is generated by the \"Tagger\"\n",
    "    * \"top_terms\" are pregenerated keywords for each sample, extracted from the the context around each sample using the \"KeyBERT\"\n",
    "    * \"label\" for candidates will be same as existing LSF or non-LSF samples if they have a close neighbour from them otherwise candidates remain unlabeled (-1 for BERTopic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building the index with 100 trees...\n",
      "Index is successfully built.\n",
      "Building the index with 100 trees...\n",
      "Index is successfully built.\n"
     ]
    }
   ],
   "source": [
    "df_context_all=pd.read_csv('../../data/Final_Context.tsv',sep='\\t')\n",
    "\n",
    "index_lsf_names=df_context_all[df_context_all.serial< 200000].name.tolist()\n",
    "index_lsf_labels=df_context_all[df_context_all.serial< 200000].label.tolist()\n",
    "\n",
    "index_non_lsf_names=df_context_all[(df_context_all.serial>= 200000) &  (df_context_all.serial< 300000)].name.tolist()\n",
    "index_non_lsf_labels=df_context_all[(df_context_all.serial>= 200000) &  (df_context_all.serial< 300000)].label.tolist()\n",
    "\n",
    "index_lsf=semantic_term_placement.build_annoy_index(index_lsf_names)\n",
    "index_non_lsf=semantic_term_placement.build_annoy_index(index_non_lsf_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_by_KNN (candidates,index_lsf,index_lsf_names,index_non_lsf,index_non_lsf_names):\n",
    "    probs_index=[]\n",
    "    for i,name in enumerate(candidates):\n",
    "        \n",
    "        _,ditance_lsf=semantic_term_placement.find_neighbors(index_lsf,index_lsf_names, query_name =name,num_matches=3)\n",
    "        _,distance_non_lsf=semantic_term_placement.find_neighbors(index_non_lsf,index_non_lsf_names,query_name=name,num_matches=3)\n",
    "\n",
    "        score_lsf=1/np.mean(ditance_lsf)**2\n",
    "  \n",
    "        score_nonLSF=1/np.mean(distance_non_lsf)**2\n",
    "      \n",
    "        sum_scores=sum([score_lsf,score_nonLSF])\n",
    "        probs_index.append(score_lsf/sum_scores)\n",
    "    return probs_index\n",
    "\n",
    "probs_KNN=predict_by_KNN(test_candidates_names,index_lsf,index_lsf_names,index_non_lsf,index_non_lsf_names)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict using BERTopic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "def load_and_predict_by_BERTopic_model(model_path,test_candidates_context):\n",
    "    BERTopic_model = BERTopic.load(model_path)\n",
    "    # these topics are selected manually and it should be changed if the model is retrianed\n",
    "    LSF_topics=[3,4,6,8,9,10,13,14,17,18,19,20,21,23,25,27,30,31,33,36,39,40,45]\n",
    "    predicted_topics=[]\n",
    "    # Predict test samples\n",
    "    predicted_probs=[]\n",
    "    for doc in test_candidates_context:\n",
    "\n",
    "        topics,probs=BERTopic_model.transform(doc)\n",
    "        sum_probs=sum(probs[0])\n",
    "        outlier_prob=1-sum_probs\n",
    "        prob=sum(probs[0][LSF_topics])\n",
    "        predicted_topic=np.argmax(probs[0])\n",
    "        if topics[0] in LSF_topics:\n",
    "            prob=prob+outlier_prob\n",
    "        predicted_probs.append(prob)\n",
    "        predicted_topics.append(np.argmax(probs[0]))\n",
    "    predicted_probs=np.array(predicted_probs)\n",
    "    return BERTopic_model,predicted_probs\n",
    "\n",
    "\n",
    "#model_path=\"/Users/dzq660/LOCAL/LSF_Ontology/Trained_Topic_Models/model_unsupervised_guided\"\n",
    "model_path='../../model/BERTopic_Model'\n",
    "BERTopic_model,BERTopic_predictions=load_and_predict_by_BERTopic_model(model_path,test_candidates_context)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Consensus of BERTopic and KNN using calibrated probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* To callibrate probabilities we have used approaches such as isotonic callibration and manual function fit, here for simplity linear regression is provided"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def prepare_data(labels,probs,treshold):\n",
    "    #TN=np.zeros(shape=(len(all_names_candidates_test)))\n",
    "    TP=np.zeros(shape=(len(labels)))\n",
    "    #FN=np.zeros(shape=(len(all_names_candidates_test)))\n",
    "    FP=np.zeros(shape=(len(labels)))\n",
    "    \n",
    "    for i,label in enumerate(labels):\n",
    "        if probs[i]>=treshold and label==1:\n",
    "            TP[i]=1\n",
    "        elif probs[i]>= treshold and label==0:\n",
    "            FP[i]=1\n",
    "      \n",
    "    df=pd.DataFrame({'score':probs,'TP':TP,'FP':FP})\n",
    "    df=df[(df.TP==1) | (df.FP==1)]\n",
    "    df=df.sort_values(by=['score'],ascending=False)\n",
    "    #df=df.sort_values(by=['score'])\n",
    "    return df\n",
    "\n",
    "\n",
    "def produce_curve_data(labels,probs,treshold,window_size):\n",
    "\toutput = []\n",
    "\twindow_check = False\n",
    "\tscore_sum = 0\n",
    "\ttp_sum = 0\n",
    "\tfp_sum = 0\n",
    "\n",
    "\tdf=prepare_data(labels,probs,treshold)\n",
    "\n",
    "\tpair_window = deque(maxlen = window_size)\n",
    "\n",
    "\tfor score_in,tp_in,fp_in in df.itertuples(index=False):\n",
    "\n",
    "\n",
    "\t\t# retrieve values that will leave the window\n",
    "\t\tif len(pair_window) > 0 and window_check == True:\n",
    "\t\t\t(score_out, tp_out, fp_out) = pair_window[0]\n",
    "\t\telse:\n",
    "\t\t\tscore_out = tp_out = fp_out = 0\n",
    "\n",
    "\t\t# calculate current sums within window\n",
    "\t\tscore_sum += score_in - score_out\n",
    "\t\ttp_sum += tp_in - tp_out\n",
    "\t\tfp_sum += fp_in - fp_out\n",
    "\n",
    "\t\t# slide window\n",
    "\t\tpair_window += [[score_in, tp_in, fp_in]]\n",
    "\n",
    "\t\tif len(pair_window) == window_size:\n",
    "\t\t\t# calculate score average and precision within window\n",
    "\t\t\tscore_av = score_sum / window_size\n",
    "\t\t\tif (tp_sum + fp_sum)==0:\n",
    "\t\t\t\tprecision=0\n",
    "\t\t\telse:\n",
    "\t\t\t\tprecision = tp_sum / (tp_sum + fp_sum)   \n",
    "\t\t\toutput += [[score_av, precision]]\n",
    "\t\t\n",
    "\t\t\twindow_check = True\n",
    "\n",
    "\tscore_av=[]\n",
    "\tprecision=[]\n",
    "\tfor x,y in output:\n",
    "\t\tscore_av.append(x)\n",
    "\t\tprecision.append(y)\n",
    "\tprecision=np.array(precision)\n",
    "\tscore_av=np.array(score_av)\n",
    "\treturn (score_av,precision)\n",
    "\n",
    "\n",
    "def calibrate_linear(x,y,x_test):\n",
    "    # Linear regression model\n",
    "    gradient, intercept, r_value, p_value, slope_std_error = stats.linregress(x, y)\n",
    "\n",
    "    # Line of best fit\n",
    "    predict_y = gradient * x_test + intercept\n",
    "\n",
    "    return predict_y\n",
    "\n",
    "\n",
    "\n",
    "# We calibrate probs which are larger than 0.56, less than this value can be replaced by ratio of pos:neg  which is 0.3\n",
    "def calibrate_Index(x,y,x_test):\n",
    "\n",
    "    # Linear regression model\n",
    "    gradient, intercept, r_value, p_value, slope_std_error = stats.linregress(x, y)\n",
    "\n",
    "    # Line of best fit\n",
    "    Y=[]\n",
    "    for x in x_test:\n",
    "        y=gradient * x + intercept\n",
    "        if y<0.3:\n",
    "            y=0.3\n",
    "        elif y>1:\n",
    "            y=1\n",
    "        Y.append(y)\n",
    "    Y=np.array(Y)\n",
    "    return Y\n",
    "\n",
    "\n",
    "def consensus_index_context_calibrated(probs_index_callibrated,probs_context_callibrated):\n",
    "    \n",
    "    \n",
    "    return 1- (1-probs_index_callibrated)*(1-probs_context_callibrated)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "xData_KNN,yData_KNN=produce_curve_data(binary_labels,probs_KNN,0.5,window_size=50)\n",
    "\n",
    "probs_KNN_calibrated=calibrate_Index(xData_KNN,yData_KNN,probs_KNN)\n",
    "\n",
    "xData_BERTopic,yData_BERToic=produce_curve_data(binary_labels,BERTopic_predictions,0.0,window_size=50)\n",
    "probs_BERTopic_calibrated=calibrate_linear(xData_BERTopic,yData_BERToic, BERTopic_predictions)\n",
    "probs_consensus_KNN_BERTOpic_calibrated=consensus_index_context_calibrated(probs_KNN_calibrated,probs_BERTopic_calibrated) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# display = PrecisionRecallDisplay.from_predictions(binary_labels, probs_BERTopic_calibrated,ax= plt.gca(), name=\"BERTopic-Context\")\n",
    "\n",
    "# _ = PrecisionRecallDisplay.from_predictions(binary_labels, probs_consensus_KNN_BERTOpic_calibrated,ax= plt.gca(), name=\"Consensus\",color='red')\n",
    "\n",
    "# _ = PrecisionRecallDisplay.from_predictions(binary_labels, probs_KNN_calibrated,ax= plt.gca(), name=\"Index\",color='green')\n",
    "\n",
    "\n",
    "\n",
    "# _ = display.ax_.set_title(\"Precision-Recall curve - (Positive to Negative Ratio) < (1:\" + str(round(ratio))+')')\n",
    "\n",
    "# plt.legend(loc='upper right')\n",
    "# #plt.legend(loc='lower left')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lsf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
